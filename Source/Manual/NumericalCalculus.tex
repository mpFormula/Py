
\chapter{Polynomials}
See also taylor() and chebyfit() for approximation of functions by polynomials.

\section{Polynomial evaluation}


\begin{mpFunctionsExtract}
	\mpFunctionThree
	{polyval? mpNum? a polynomial }
	{coeffs? mpNum? A list of coefficients (real or complex numbers).}
	{x? mpNum? A real or complex number.}	
	{Keywords? String? derivative=False.}	
\end{mpFunctionsExtract}




\vpara
Given coefficients $[c_n,\ldots,c_2,c_1,c_0]$ and a number $x$, polyval() evaluates the polynomial 

\begin{equation}
P(x)=c_n x^n + \ldots + c_2 x^2 + c_1 x + c_0
\end{equation}

If derivative=True is set, polyval() simultaneously evaluates $P(x)$ with the derivative, $P'(x)$, and returns the tuple $(P(x),P'(x))$.

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.pretty = True
>>> polyval([3, 0, 2], 0.5)
2.75
>>> polyval([3, 0, 2], 0.5, derivative=True)
(2.75, 3.0)
\end{lstlisting}

The coefficients and the evaluation point may be any combination of real or complex numbers.


\newpage
\section{Polynomial roots}


\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{polyroots? mpNum? all roots (real or complex) of a given polynomial.}
	{coeffs? mpNum? A real or complex number.}
	{Keywords? String? maxsteps=50, cleanup=True, extraprec=10, error=False.}	
\end{mpFunctionsExtract}


\vpara
The roots are returned as a sorted list, where real roots appear first followed by complex conjugate roots as adjacent elements. The polynomial should be given as a list of coefficients, in the format used by polyval(). The leading coefficient must be nonzero.

With error=True, polyroots() returns a tuple (roots, err) where err is an estimate of the maximum error among the computed roots.

\vpara
Examples

Finding the three real roots of $x^3-x^2-14x+24$:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>> nprint(polyroots([1,-1,-14,24]), 4)
[-4.0, 2.0, 3.0]
\end{lstlisting}

Finding the two complex conjugate roots of $4x^2+3x+2$, with an error estimate:

\lstset{language={Python}}
\begin{lstlisting}
>>> roots, err = polyroots([4,3,2], error=True)
>>> for r in roots:
... print(r)
...
(-0.375 + 0.59947894041409j)
(-0.375 - 0.59947894041409j)
>>>
>>> err
2.22044604925031e-16
>>>
>>> polyval([4,3,2], roots[0])
(2.22044604925031e-16 + 0.0j)
>>> polyval([4,3,2], roots[1])
(2.22044604925031e-16 + 0.0j)
\end{lstlisting}

The following example computes all the 5th roots of unity; that is, the roots of $x^5-1$:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 20
>>> for r in polyroots([1, 0, 0, 0, 0, -1]):
... print(r)
...
1.0
(-0.8090169943749474241 + 0.58778525229247312917j)
(-0.8090169943749474241 - 0.58778525229247312917j)
(0.3090169943749474241 + 0.95105651629515357212j)
(0.3090169943749474241 - 0.95105651629515357212j)
\end{lstlisting}

Precision and conditioning

The roots are computed to the current working precision accuracy. If this accuracy cannot be achieved in \textit{maxsteps} steps, then a \textit{NoConvergence} exception is raised.

The algorithm internally is using the current working precision extended by \textit{extrapec}. If \textit{NoConvergence} was raised, that is caused either by not having enough extra precision to achieve convergence (in which case increasing \textit{extrapec} should fix the
problem) or too low \textit{maxsteps} (in which case increasing \textit{maxsteps} should fix the problem), or a combination of both.

The user should always do a convergence study with regards to \textit{extrapec} to ensure accurate results. It is possible to get convergence to a wrong answer with too low \textit{extrapec}.

Provided there are no repeated roots, polyroots() can typically compute all roots of an arbitrary polynomial to high precision:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 60
>>> for r in polyroots([1, 0, -10, 0, 1]):
... print r
...
-3.14626436994197234232913506571557044551247712918732870123249
-0.317837245195782244725757617296174288373133378433432554879127
0.317837245195782244725757617296174288373133378433432554879127
3.14626436994197234232913506571557044551247712918732870123249
>>>
>>> sqrt(3) + sqrt(2)
3.14626436994197234232913506571557044551247712918732870123249
>>> sqrt(3) - sqrt(2)
0.317837245195782244725757617296174288373133378433432554879127
\end{lstlisting}

Algorithm

polyroots() implements the Durand-Kerner method [1], which uses complex
arithmetic to locate all roots simultaneously. The Durand-Kerner method can be viewed as approximately performing simultaneous Newton iteration for all the roots. In particular, the convergence to simple roots is quadratic, just like Newton's method.

Although all roots are internally calculated using complex arithmetic, any root found to have an imaginary part smaller than the estimated numerical error is truncated to a real number (small real parts are also chopped). Real roots are placed first in the returned list, sorted by value. The remaining complex roots are sorted by their real parts so that conjugate roots end up next to each other.




\newpage
\chapter{Root-finding and optimization}

\section{Root-finding}


\begin{mpFunctionsExtract}
	\mpFunctionThree
	{findroot? mpNum?  a solution to $f(x)=0$, using \textit{x0} as starting point or interval for \textit{x}.}
	{f? mpNum? A one dimensional function}
	{x0? mpNum? A real or complex number.}	
	{Keywords? String? solver=Secant, tol=None, verbose=False, verify=True. Many more, see below.}	
\end{mpFunctionsExtract}

\vpara
Multidimensional overdetermined systems are supported. You can specify them using a function or a list of functions.

If the found root does not satisfy $|f(x)^2< \text{tol}|$, an exception is raised (this can be disabled with verify=False).

\vpara
\textbf{Arguments}

\vpara
\textit{f}: one dimensional function

\vpara
\textit{x0}: starting point, several starting points or interval (depends on solver)

\vpara
\textit{tol}: the returned solution has an error smaller than this

\vpara
\textit{verbose}: print additional information for each iteration if true

\vpara
\textit{verify}: verify the solution and raise a ValueError if

\vpara
\textit{solver}: a generator for f and x0 returning approximative solution and error

\vpara
\textit{maxsteps}: after how many steps the solver will cancel

\vpara
\textit{df}: first derivative of f (used by some solvers)

\vpara
\textit{d2f}: second derivative of f (used by some solvers)

\vpara
\textit{multidimensional}: force multidimensional solving

\vpara
\textit{J}: Jacobian matrix of f (used by multidimensional solvers)

\vpara
\textit{norm}: used vector norm (used by multidimensional solvers)

\vpara
solver has to be callable with (f, x0, **kwargs) and return an generator yielding pairs of approximative solution and estimated error (which is expected to be positive).

\vpara
You can use the following string aliases: 'secant', 'mnewton', 'halley', 'muller', 'illinois', 'pegasus', 'anderson', 'ridder', 'anewton', 'bisect'. See mpFormulaPy.calculus.optimization for their documentation.

\vpara
\textbf{Examples}

\vpara
The function findroot() locates a root of a given function using the secant method by default. A simple example use of the secant method is to compute $\pi$ as the root of $\sin x$ closest to $x_0=3$:


\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 30; mp.pretty = True
>>> findroot(sin, 3)
3.14159265358979323846264338328
\end{lstlisting}

The secant method can be used to find complex roots of analytic functions, although it must in that case generally be given a nonreal starting value (or else it will never leave the real line):

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> findroot(lambda x: x**3 + 2*x + 1, j)
(0.226698825758202 + 1.46771150871022j)
\end{lstlisting}



A nice application is to compute nontrivial roots of the Riemann zeta function with many digits (good initial values are needed for convergence):

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 30
>>> findroot(zeta, 0.5+14j)
(0.5 + 14.1347251417346937904572519836j)
\end{lstlisting}


The secant method can also be used as an optimization algorithm, by passing it a derivative of a function. The following example locates the positive minimum of the gamma function:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 20
>>> findroot(lambda x: diff(gamma, x), 1)
1.4616321449683623413
\end{lstlisting}

Finally, a useful application is to compute inverse functions, such as the Lambert W function which is the inverse of $w e^w$, given the first term of the solution's asymptotic expansion as the initial value. In basic cases, this gives identical results to mpFormulaPy's built-in lambertw function:

\lstset{language={Python}}
\begin{lstlisting}
>>> def lambert(x):
... return findroot(lambda w: w*exp(w) - x, log(1+x))
...
>>> mp.dps = 15
>>> lambert(1); lambertw(1)
0.567143290409784
0.567143290409784
>>> lambert(1000); lambert(1000)
5.2496028524016
5.2496028524016
\end{lstlisting}

Multidimensional functions are also supported:

\lstset{language={Python}}
\begin{lstlisting}
>>> f = [lambda x1, x2: x1**2 + x2,
... lambda x1, x2: 5*x1**2 - 3*x1 + 2*x2 - 3]
>>> findroot(f, (0, 0))
[-0.618033988749895]
[-0.381966011250105]
>>> findroot(f, (10, 10))
[ 1.61803398874989]
[-2.61803398874989]
\end{lstlisting}

You can verify this by solving the system manually.

Please note that the following (more general) syntax also works:

\lstset{language={Python}}
\begin{lstlisting}
>>> def f(x1, x2):
... return x1**2 + x2, 5*x1**2 - 3*x1 + 2*x2 - 3
...
>>> findroot(f, (0, 0))
[-0.618033988749895]
[-0.381966011250105]
\end{lstlisting}

\textbf{Multiple roots}

For multiple roots all methods of the Newtonian family (including secant) converge slowly. Consider this example:

\lstset{language={Python}}
\begin{lstlisting}
>>> f = lambda x: (x - 1)**99
>>> findroot(f, 0.9, verify=False)
0.918073542444929
\end{lstlisting}

Even for a very close starting point the secant method converges very slowly. Use verbose=True to illustrate this.

It is possible to modify Newton's method to make it converge regardless of the root's multiplicity:

\lstset{language={Python}}
\begin{lstlisting}
>>> findroot(f, -10, solver="mnewton")
1.0
\end{lstlisting}

This variant uses the first and second derivative of the function, which is not very efficient. 

Alternatively you can use an experimental Newtonian solver that keeps track of the speed of convergence and accelerates it using Steffensen's method if necessary:

\lstset{language={Python}}
\begin{lstlisting}
>>> findroot(f, -10, solver="anewton", verbose=True)
x: -9.88888888888888888889
error: 0.111111111111111111111
converging slowly
x: -9.77890011223344556678
error: 0.10998877665544332211
converging slowly
x: -9.67002233332199662166
error: 0.108877778911448945119
converging slowly
accelerating convergence
x: -9.5622443299551077669
error: 0.107778003366888854764
converging slowly
x: 0.99999999999999999214
error: 10.562244329955107759
x: 1.0
error: 7.8598304758094664213e-18
ZeroDivisionError: canceled with x = 1.0
1.0
\end{lstlisting}

\textbf{Complex roots}

For complex roots it is recommended to use Muller's method as it converges very quickly even for real starting points :

\lstset{language={Python}}
\begin{lstlisting}
>>> findroot(lambda x: x**4 + x + 1, (0, 1, 2), solver="muller")
(0.727136084491197 + 0.934099289460529j)
\end{lstlisting}

\textbf{Intersection methods}

When you need to find a root in a known interval, it is highly recommended to use an intersection-based solver like 'anderson' or 'ridder'. Usually they converge faster and more reliable. They have however problems with multiple roots and usually need a sign change to find a root:

\lstset{language={Python}}
\begin{lstlisting}
>>> findroot(lambda x: x**3, (-1, 1), solver="anderson")
0.0
\end{lstlisting}


Be careful with symmetric functions:

\lstset{language={Python}}
\begin{lstlisting}
>>> findroot(lambda x: x**2, (-1, 1), solver="anderson")
Traceback (most recent call last):
...
ZeroDivisionError
\end{lstlisting}

It fails even for better starting points, because there is no sign change:

\lstset{language={Python}}
\begin{lstlisting}
>>> findroot(lambda x: x**2, (-1, .5), solver='anderson')
Traceback (most recent call last):
...
ValueError: Could not find root within given tolerance. (1 > 2.1684e-19)
Try another starting point or tweak arguments.
\end{lstlisting}


\newpage
\section{Solvers}

\subsection{Secant}
1d-solver generating pairs of approximative root and error.
Needs starting points x0 and x1 close to the root. x1 defaults to x0 + 0.25.

Pro: converges quickly

Contra:  converges slowly for multiple roots


\subsection{Newton}
1d-solver generating pairs of approximative root and error.
Needs starting points x0 close to the root.

Pro: converges fast. Sometimes more robust than secant with bad second starting point

Contra:  converges slowly for multiple roots. Needs first derivative.
 2 function evaluations per iteration


\subsection{MNewton}
1d-solver generating pairs of approximative root and error.
Needs starting point x0 close to the root. Uses modified Newton's method that converges fast regardless of the multiplicity of the root.

Pro: converges quickly for multiple roots.

Contra:  needs first and second derivative of f. 3 function evaluations per iteration



\subsection{Halley}
1d-solver generating pairs of approximative root and error. Needs a starting point x0 close to the root. Uses Halley's method with cubic
convergence rate.

Pro: converges even faster than Newton's method. Useful when computing with many digits

Contra:  needs first and second derivative of f. 3 function evaluations per iteration. Converges slowly for multiple roots




\subsection{Muller}
1d-solver generating pairs of approximative root and error.
Needs starting points x0, x1 and x2 close to the root. x1 defaults to x0 + 0.25; x2 to x1 + 0.25. Uses Muller's method that converges towards complex roots.

Pro: converges quickly (somewhat faster than secant). Can find complex roots

Contra:  converges slowly for multiple roots. May have complex values for real starting points and real roots.



\subsection{Bisection}
1d-solver generating pairs of approximative root and error.
Uses bisection method to find a root of f in [a, b]. Might fail for multiple roots (needs sign change).

Pro: robust and reliable

Contra:  converges slowly. Needs sign change


\subsection{Illinois}
1d-solver generating pairs of approximative root and error.

Uses Illinois method or similar to find a root of f in [a, b]. Might fail for multiple roots (needs sign change). Combines bisect with secant (improved regula falsi).

The only difference between the methods is the scaling factor m, which is used to ensure convergence (you can choose one using the 'method' keyword):

\vpara
Illinois method ('illinois'): $m = 0.5$

Pegasus method ('pegasus'): $m = fb/(fb + fz)$

Anderson-Bjoerk method ('anderson'): $m = 1 - fz/fb$ if positive else 0.5

\vpara
Pro: converges very fast

Contra:  has problems with multiple roots. Needs sign change




\subsection{Pegasus}
1d-solver generating pairs of approximative root and error.

Uses Pegasus method to find a root of f in [a, b]. 

Wrapper for illinois to use method='pegasus'.



\subsection{Anderson}
1d-solver generating pairs of approximative root and error.

Uses Anderson-Bjoerk method to find a root of f in [a, b]. 

Wrapper for illinois to use method='anderson'.



\subsection{Ridder}
1d-solver generating pairs of approximative root and error.
Ridders' method to find a root of f in [a, b]. Is told to perform as well as Brent's method while being simpler.

\vpara
Pro: very fast. Simpler than Brent's method

Contra: two function evaluations per step. Has problems with multiple roots. Needs sign change




\subsection{ANewton}
EXPERIMENTAL 1d-solver generating pairs of approximative root and error.

Uses Newton's method modified to use Steffensens method when convergence is slow (i.e. for multiple roots.)



\subsection{MDNewton}
Find the root of a vector function numerically using Newton's method.

f is a vector function representing a nonlinear equation system.

x0 is the starting point close to the root.

J is a function returning the Jacobian matrix for a point.

Supports overdetermined systems.

\vpara
Use the 'norm' keyword to specify which norm to use. Defaults to max-norm. The function to calculate the Jacobian matrix can be given using the keyword 'J'. Otherwise it will be calculated numerically.

\vpara
Please note that this method converges only locally. Especially for high- dimensional systems it is not trivial to find a good starting point being close enough to the root.

\vpara
It is recommended to use a faster, low-precision solver from SciPy [1] or OpenOpt [2] to get an initial guess. Afterwards you can use this method for root-polishing to any precision.




\newpage
\chapter{Sums, products, limits and extrapolation}
The functions listed here permit approximation of infinite sums, products, and other sequence limits. Use mpFormulaPy.fsum() and mpFormulaPy.fprod() for summation and multiplication of finite sequences.


\section{Summation}

%\subsection{nsum(ctx, f, *intervals, **options)}

\subsection{One-dimensional Summation}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{nsum? mpNum? a one dimensional (possibly infinite) sum.}
	{f? mpNum? A one dimensional function}
	{interval? mpNum? A real interval.}	
	{Keywords? String? method=r+s, tol=eps, verbose=False, maxterms=10*dps. Many more, see below.}	
\end{mpFunctionsExtract}

\vpara
This function computes the sum

\begin{equation}
S = \sum_{k=a}^b f(k)
\end{equation}

where $(a,b)$ = interval, and where $a=-\infty$ and/or $b=\infty$ are allowed, or more generally

\begin{equation}
S = \sum_{k_1=a_1}^{b_1} \cdots \sum_{k_n=a_n}^{b_n} f(k_1,\ldots,k_n)
\end{equation}

if multiple intervals are given.

Two examples of infinite series that can be summed by nsum(), where the first converges rapidly and the second converges slowly, are:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>> nsum(lambda n: 1/fac(n), [0, inf])
2.71828182845905
>>> nsum(lambda n: 1/n**2, [1, inf])
1.64493406684823
\end{lstlisting}

When appropriate, nsum() applies convergence acceleration to accurately estimate the sums of slowly convergent series. If the series is finite, nsum() currently does not attempt to perform any extrapolation, and simply calls fsum().

Multidimensional infinite series are reduced to a single-dimensional series over expanding hypercubes; if both infinite and finite dimensions are present, the finite ranges are moved innermost. For more advanced control over the summation order, use nested calls to nsum(), or manually rewrite the sum as a single-dimensional series.

\vpara
\textbf{Options}

\vpara
\textit{tol}: Desired maximum final error. Defaults roughly to the epsilon of the working precision.

\vpara
\textit{method}: Which summation algorithm to use (described below). Default: 'richardson+shanks'.

\vpara
\textit{maxterms}: Cancel after at most this many terms. Default: 10*dps.

\vpara
\textit{steps}: An iterable giving the number of terms to add between each extrapolation attempt. The default sequence is [10, 20, 30, 40, ...]. For example, if you know that approximately 100 terms will be required, efficiency might be improved by setting this to [100, 10]. Then the first extrapolation will be performed after 100 terms, the second after 110, etc.

\vpara
\textit{verbose}: Print details about progress.

\vpara
\textit{ignore}: If enabled, any term that raises ArithmeticError or ValueError (e.g. through division by zero) is replaced by a zero. This is convenient for lattice sums with a singular term near the origin.

\vpara
\textbf{Methods}

Unfortunately, an algorithm that can efficiently sum any infinite series does not exist. nsum() implements several different algorithms that each work well in different cases.

The method keyword argument selects a method.

\vpara
The default method is 'r+s', i.e. both Richardson extrapolation and Shanks transformation is attempted. A slower method that handles more cases is 'r+s+e'. For very high precision summation, or if the summation needs to be fast (for example if multiple sums need to be evaluated), it is a good idea to investigate which one method works best and only use that.

\vpara
'richardson' / 'r':

Uses Richardson extrapolation. Provides useful extrapolation when $f(k) \sim P(k)/Q(k)$ or when $f(k) \sim (-1)^k P(k)/Q(k)$ for polynomials $P$ and $Q$.

See richardson() for additional information.


\vpara
'shanks' / 's':

Uses Shanks transformation. Typically provides useful extrapolation when $f(k) \sim c^k$ or when successive terms alternate signs. Is able to sum some
divergent series. See shanks() for additional information.


\vpara
'levin' / 'l':

Uses the Levin transformation. It performs better than the Shanks transformation for logarithmic convergent or alternating divergent series. The 'levin\_variant'-keyword selects the variant. Valid choices are "u", "t", "v" and "all" whereby "all" uses all three u,t and v simultanously (This is good for performance comparison in conjunction with "verbose=True"). Instead of the Levin transform one can also use the Sidi-S transform by selecting the method 'sidi'. See levin() for additional details.


\vpara
'alternating' / 'a':

This is the convergence acceleration of alternating series developped by Cohen, Villegras and Zagier. See cohen\_alt() for additional details.


\vpara
'euler-maclaurin' / 'e':

Uses the Euler-Maclaurin summation formula to approximate the remainder sum by an integral. This requires high-order numerical derivatives and numerical integration. The advantage of this algorithm is that it works regardless of the decay rate of $f$, as long as $f$ is sufficiently smooth. See sumem() for additional information.


\vpara
'direct' / 'd':

Does not perform any extrapolation. This can be used (and should only be used for) rapidly convergent series. The summation automatically stops when the terms decrease below the target tolerance.

\vpara
\textbf{Basic examples}

A finite sum:

\lstset{language={Python}}
\begin{lstlisting}
>>> nsum(lambda k: 1/k, [1, 6])
2.45
\end{lstlisting}


Summation of a series going to negative infinity and a doubly infinite series:

\lstset{language={Python}}
\begin{lstlisting}
>>> nsum(lambda k: 1/k**2, [-inf, -1])
1.64493406684823
>>> nsum(lambda k: 1/(1+k**2), [-inf, inf])
3.15334809493716
\end{lstlisting}

nsum() handles sums of complex numbers:

\lstset{language={Python}}
\begin{lstlisting}
>>> nsum(lambda k: (0.5+0.25j)**k, [0, inf])
(1.6 + 0.8j)
\end{lstlisting}


The following sum converges very rapidly, so it is most efficient to sum it by disabling convergence acceleration:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 1000
>>> a = nsum(lambda k: -(-1)**k * k**2 / fac(2*k), [1, inf], method="direct")
>>> b = (cos(1)+sin(1))/4
>>> abs(a-b) < mpf('1e-998')
True
\end{lstlisting}

\vpara
\textbf{Examples with Richardson extrapolation}

Richardson extrapolation works well for sums over rational functions, as well as their alternating counterparts:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 50
>>> nsum(lambda k: 1 / k**3, [1, inf], method="richardson")
1.2020569031595942853997381615114499907649862923405
>>> zeta(3)
1.2020569031595942853997381615114499907649862923405
>>> nsum(lambda n: (n + 3)/(n**3 + n**2), [1, inf], method="richardson")
2.9348022005446793094172454999380755676568497036204
>>> pi**2/2-2
2.9348022005446793094172454999380755676568497036204
>>> nsum(lambda k: (-1)**k / k**3, [1, inf], method="richardson")
-0.90154267736969571404980362113358749307373971925537
>>> -3*zeta(3)/4
-0.90154267736969571404980362113358749307373971925538
\end{lstlisting}

\vpara
\textbf{Examples with Shanks extrapolation}

The Shanks transformation works well for geometric series and typically provides excellent acceleration for Taylor series near the border of their disk of convergence. Here we apply it to a series for $\log(2)$, which can be seen as the Taylor series for $\log(1+x)$ with $x=1$:

\lstset{language={Python}}
\begin{lstlisting}
>>> nsum(lambda k: -(-1)**k/k, [1, inf], method="shanks")
0.69314718055994530941723212145817656807550013436025
>>> log(2)
0.69314718055994530941723212145817656807550013436025
\end{lstlisting}

Here we apply it to a slowly convergent geometric series:

\lstset{language={Python}}
\begin{lstlisting}
>>> nsum(lambda k: mpf("0.995")**k, [0, inf], method="shanks")
200.0
\end{lstlisting}

Finally, Shanks' method works very well for alternating series where $f(k)=(-1)^k g(k)$, and often does so regardless of the exact decay rate of $g(k)$:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> nsum(lambda k: (-1)**(k+1) / k**1.5, [1, inf], method="shanks")
0.765147024625408
>>> (2-sqrt(2))*zeta(1.5)/2
0.765147024625408
\end{lstlisting}

The following slowly convergent alternating series has no known closed-form value. Evaluating the sum a second time at higher precision indicates that the value is probably correct:

\lstset{language={Python}}
\begin{lstlisting}
>>> nsum(lambda k: (-1)**k / log(k), [2, inf], method="shanks")
0.924299897222939
>>> mp.dps = 30
>>> nsum(lambda k: (-1)**k / log(k), [2, inf], method="shanks")
0.92429989722293885595957018136
\end{lstlisting}

\vpara
\textbf{Examples with Levin transformation}

The following example calculates Eulerâ€™s constant as the constant term in the Laurent expansion of zeta(s) at s=1. This sum converges extremly slow because of the logarithmic convergence behaviour of the Dirichlet series for zeta.


\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 30
>>> z = mp.mpf(10) ** (-10)
>>> a = mp.nsum(lambda n: n**(-(1+z)), [1, mp.inf], method = "levin") - 1 / z
>>> print(mp.chop(a - mp.euler, tol = 1e-10))
0.0
\end{lstlisting}

Now we sum the zeta function outside its range of convergence (attention: This does not work at the negative integers!):


\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> w = mp.nsum(lambda n: n ** (2 + 3j), [1, mp.inf], method = "levin", levin_variant = "v")
>>> print(mp.chop(w - mp.zeta(-2-3j)))
0.0
\end{lstlisting}

The next example resummates an asymptotic series expansion of an integral related to the exponential integral.


\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> z = mp.mpf(10)
>>> # exact = mp.quad(lambda x: mp.exp(-x)/(1+x/z),[0,mp.inf])
>>> exact = z * mp.exp(z) * mp.expint(1,z) # this is the symbolic expression for the integral
>>> w = mp.nsum(lambda n: (-1) ** n * mp.fac(n) * z ** (-n), [0, mp.inf], method = "sidi", levin_variant = "t")
>>> print(mp.chop(w - exact))
0.0
\end{lstlisting}

Following highly divergent asymptotic expansion needs some care. Firstly we need copious amount of working precision. Secondly the stepsize must not be chosen too large, otherwise nsum may miss the point where the Levin transform converges and reach the point where only numerical garbage is produced due to numerical cancellation.


\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> z = mp.mpf(2)
>>> # exact = mp.quad(lambda x: mp.exp( -x * x / 2 - z * x ** 4), [0,mp.inf]) * 2 / mp.sqrt(2 * mp.pi)
>>> exact = mp.exp(mp.one / (32 * z)) * mp.besselk(mp.one / 4, mp.one / (32 * z)) / (4 * mp.sqrt(z * mp.pi)) # this is the symbolic expression for the integral
>>> w = mp.nsum(lambda n: (-z)**n * mp.fac(4 * n) / (mp.fac(n) * mp.fac(2 * n) * (4 ** n)),
...   [0, mp.inf], method = "levin", levin_variant = "t", workprec = 8*mp.prec, steps = [2] + [1 for x in xrange(1000)])
>>> print(mp.chop(w - exact))
0.0
\end{lstlisting}

The hypergeometric function can also be summed outside its range of convergence:


\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> z = 2 + 1j
>>> exact = mp.hyp2f1(2 / mp.mpf(3), 4 / mp.mpf(3), 1 / mp.mpf(3), z)
>>> f = lambda n: mp.rf(2 / mp.mpf(3), n) * mp.rf(4 / mp.mpf(3), n) * z**n / (mp.rf(1 / mp.mpf(3), n) * mp.fac(n))
>>> v = mp.nsum(f, [0, mp.inf], method = "levin", steps = [10 for x in xrange(1000)])
>>> print(mp.chop(exact-v))
0.0
\end{lstlisting}

\vpara
\textbf{Examples with Cohen's alternating series resummation}

The next example sums the alternating zeta function:


\lstset{language={Python}}
\begin{lstlisting}
>>> v = mp.nsum(lambda n: (-1)**(n-1) / n, [1, mp.inf], method = "a")
>>> print(mp.chop(v - mp.log(2)))
0.0
\end{lstlisting}

The derivate of the alternating zeta function outside its range of
convergence:

\lstset{language={Python}}
\begin{lstlisting}
>>> v = mp.nsum(lambda n: (-1)**n * mp.log(n) * n, [1, mp.inf], method = "a")
>>> print(mp.chop(v - mp.diff(lambda s: mp.altzeta(s), -1)))
0.0
\end{lstlisting}

\vpara
\textbf{Examples with Euler-Maclaurin summation}

The sum in the following example has the wrong rate of convergence for either Richardson or Shanks to be effective.


\lstset{language={Python}}
\begin{lstlisting}
>>> f = lambda k: log(k)/k**2.5
>>> mp.dps = 15
>>> nsum(f, [1, inf], method="euler-maclaurin")
0.38734195032621
>>> -diff(zeta, 2.5)
0.38734195032621
\end{lstlisting}

Increasing steps improves speed at higher precision:

\lstset{language={Python}}
\begin{lstlisting}
>>> f = lambda k: log(k)/k**2.5
>>> mp.dps = 50
>>> nsum(f, [1, inf], method="euler-maclaurin", steps=[250])
0.38734195032620997271199237593105101319948228874688
>>> -diff(zeta, 2.5)
0.38734195032620997271199237593105101319948228874688
\end{lstlisting}

\vpara
\textbf{Divergent series}

The Shanks transformation is able to sum some divergent series. In particular, it is often able to sum Taylor series beyond their radius of convergence (this is due to a relation between the Shanks transformation and Pade approximations; see pade() for an alternative way to evaluate divergent Taylor series). Furthermore the Levintransform
examples above contain some divergent series resummation.

Here we apply it to $\log(1+x)$ far outside the region of convergence:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 50
>>> nsum(lambda k: -(-9)**k/k, [1, inf], method="shanks")
2.3025850929940456840179914546843642076011014886288
>>> log(10)
2.3025850929940456840179914546843642076011014886288
\end{lstlisting}

A particular type of divergent series that can be summed using the Shanks
transformation is geometric series. The result is the same as using the closed-form formula for an infinite geometric series:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> for n in range(-8, 8):
... if n == 1:
... continue
... print("%s %s %s" % (mpf(n), mpf(1)/(1-n),
... nsum(lambda k: n**k, [0, inf], method="shanks")))
...
-8.0 0.111111111111111 0.111111111111111
-7.0 0.125 0.125
-6.0 0.142857142857143 0.142857142857143
-5.0 0.166666666666667 0.166666666666667
-4.0 0.2 0.2
-3.0 0.25 0.25
-2.0 0.333333333333333 0.333333333333333
-1.0 0.5 0.5
0.0 1.0 1.0
2.0 -1.0 -1.0
3.0 -0.5 -0.5
4.0 -0.333333333333333 -0.333333333333333
5.0 -0.25 -0.25
6.0 -0.2 -0.2
7.0 -0.166666666666667 -0.166666666666667
\end{lstlisting}

%\vpara
%\textbf{Multidimensional sums}
%


\subsection{Two-dimensional Summation}


\begin{mpFunctionsExtract}
	\mpFunctionFour
	{nsum2d? mpNum? a two dimensional (possibly infinite) sum.}
	{f? mpNum? A one dimensional function}
	{interval1? mpNum? A real interval.}	
	{interval2? mpNum? A real interval.}		
	{Keywords? String? method=r+s, tol=eps, verbose=False, maxterms=10*dps. Many more, see below.}	
\end{mpFunctionsExtract}


\vpara
Any combination of finite and infinite ranges is allowed for the summation indices:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> nsum2d(lambda x,y: x+y, [2,3], [4,5])
28.0
>>> nsum2d(lambda x,y: x/2**y, [1,3], [1,inf])
6.0
>>> nsum2d(lambda x,y: y/2**x, [1,inf], [1,3])
6.0
>>> nsum3d(lambda x,y,z: z/(2**x*2**y), [1,inf], [1,inf], [3,4])
7.0
>>> nsum3d(lambda x,y,z: y/(2**x*2**z), [1,inf], [3,4], [1,inf])
7.0
>>> nsum3d(lambda x,y,z: x/(2**z*2**y), [3,4], [1,inf], [1,inf])
7.0
\end{lstlisting}

Some nice examples of double series with analytic solutions or reductions to single-dimensional series (see [1]):

\lstset{language={Python}}
\begin{lstlisting}
>>> nsum2d(lambda m, n: 1/2**(m*n), [1,inf], [1,inf])
1.60669515241529
>>> nsum(lambda n: 1/(2**n-1), [1,inf])
1.60669515241529
>>> nsum2d(lambda i,j: (-1)**(i+j)/(i**2+j**2), [1,inf], [1,inf])
0.278070510848213
>>> pi*(pi-3*ln2)/12
0.278070510848213
>>> nsum2d(lambda i,j: (-1)**(i+j)/(i+j)**2, [1,inf], [1,inf])
0.129319852864168
>>> altzeta(2) - altzeta(1)
0.129319852864168
>>> nsum2d(lambda i,j: (-1)**(i+j)/(i+j)**3, [1,inf], [1,inf])
0.0790756439455825
>>> altzeta(3) - altzeta(2)
0.0790756439455825
>>> nsum2d(lambda m,n: m**2*n/(3**m*(n*3**m+m*3**n)), [1,inf], [1,inf])
0.28125
>>> mpf(9)/32
0.28125
>>> nsum2d(lambda i,j: fac(i-1)*fac(j-1)/fac(i+j), [1,inf], [1,inf], workprec=400)
1.64493406684823
>>> zeta(2)
1.64493406684823
\end{lstlisting}



\subsection{Three-dimensional Summation}

\begin{mpFunctionsExtract}
	\mpFunctionFive
	{nsum3d? mpNum? a three dimensional (possibly infinite) sum .}
	{f? mpNum? A one dimensional function}
	{interval1? mpNum? A real interval.}	
	{interval2? mpNum? A real interval.}
	{interval3? mpNum? A real interval.}			
	{Keywords? String? method=r+s, tol=eps, verbose=False, maxterms=10*dps. Many more, see below.}	
\end{mpFunctionsExtract}


\vpara
A hard example of a multidimensional sum is the Madelung constant in three dimensions (see [2]). The defining sum converges very slowly and only conditionally, so nsum() is lucky to obtain an accurate value through convergence acceleration. The second evaluation below uses a much more efficient, rapidly convergent 2D sum:

\lstset{language={Python}}
\begin{lstlisting}
>>> nsum3d(lambda x,y,z: (-1)**(x+y+z)/(x*x+y*y+z*z)**0.5,
... [-inf,inf], [-inf,inf], [-inf,inf], ignore=True)
-1.74756459463318
>>> nsum2d(lambda x,y: -12*pi*sech(0.5*pi * \
... sqrt((2*x+1)**2+(2*y+1)**2))**2, [0,inf], [0,inf])
-1.74756459463318
\end{lstlisting}

Another example of a lattice sum in 2D:

\lstset{language={Python}}
\begin{lstlisting}
>>> nsum(lambda x,y: (-1)**(x+y) / (x**2+y**2), [-inf,inf],
... [-inf,inf], ignore=True)
-2.1775860903036
>>> -pi*ln2
-2.1775860903036
\end{lstlisting}

An example of an Eisenstein series:

\lstset{language={Python}}
\begin{lstlisting}
>>> nsum(lambda m,n: (m+n*1j)**(-4), [-inf,inf], [-inf,inf],
... ignore=True)
(3.1512120021539 + 0.0j)
\end{lstlisting}



\subsection{Euler-Maclaurin formula}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{sumem? mpNum? an infinite series of an analytic summand f using the Euler-Maclaurin formula}
	{f? mpNum? A one dimensional function}
	{interval? mpNum? A real interval.}	
	{Keywords? String?  tol=None, reject=10, integral=None, adiffs=None, bdiffs=None, verbose=False, error=False, fastabort=False}	
\end{mpFunctionsExtract}


\vpara
Uses the Euler-Maclaurin formula to compute an approximation accurate to within tol (which defaults to the present epsilon) of the sum

\begin{equation}
S=\sum_{k=a}^b f(k)
\end{equation}

where $(a,b)$ are given by interval and or may be infinite. The approximation is

\begin{equation}
S \sim \int_a^b f(x) dx + \frac{f(a)+f(b)}{2} + \sum_{k=1}^{\infty} \frac{B_{2k}}{(2k)!} \left(f^{(2k-1)}(b) - f^{(2k-1)(a)}  \right)
\end{equation}

The last sum in the Euler-Maclaurin formula is not generally convergent (a notable exception is if $f$ is a polynomial, in which case Euler-Maclaurin actually gives an exact result).

\vpara
The summation is stopped as soon as the quotient between two consecutive terms falls below reject. That is, by default (reject = 10), the summation is continued as long as each term adds at least one decimal.

\vpara
Although not convergent, convergence to a given tolerance can often be "forced" if $b=\infty$ by summing up to $a+N$ and then applying the Euler-Maclaurin formula to the sum over the range $(a+N+1,\ldots,\infty)$. This procedure is implemented by nsum().

\vpara
By default numerical quadrature and differentiation is used. If the symbolic values of the integral and endpoint derivatives are known, it is more efficient to pass the value of the integral explicitly as integral and the derivatives explicitly as adiffs and bdiffs.
The derivatives should be given as iterables that yield $f(a),f'(a),f''(a),\ldots$(and the equivalent for $b$).

\vpara
\textbf{Examples}

Summation of an infinite series, with automatic and symbolic integral and derivative values (the second should be much faster):

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 50; mp.pretty = True
>>> sumem(lambda n: 1/n**2, [32, inf])
0.03174336652030209012658168043874142714132886413417
>>> I = mpf(1)/32
>>> D = adiffs=((-1)**n*fac(n+1)*32**(-2-n) for n in range(999))
>>> sumem(lambda n: 1/n**2, [32, inf], integral=I, adiffs=D)
0.03174336652030209012658168043874142714132886413417
\end{lstlisting}

An exact evaluation of a finite polynomial sum:

\lstset{language={Python}}
\begin{lstlisting}
>>> sumem(lambda n: n**5-12*n**2+3*n, [-100000, 200000])
10500155000624963999742499550000.0
>>> print(sum(n**5-12*n**2+3*n for n in range(-100000, 200001)))
10500155000624963999742499550000
\end{lstlisting}



\subsection{Abel-Plana formula}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{sumap? mpNum? an infinite series of an analytic summand f using the Abel-Plana formula.}
	{f? mpNum? A one dimensional function}
	{interval? mpNum? A real interval.}	
	{Keywords? String?  integral=None, error=False}	
\end{mpFunctionsExtract}


\vpara
Evaluates an infinite series of an analytic summand f using the Abel-Plana formula

\begin{equation}
\sum_{k=0}^{\infty} f(k) = \int_0^{\infty} f(t)dt + \tfrac{1}{2}f(0) + i \int_0^{\infty} \frac{f(it)-f(-it)}{e^{2\pi t}-1}
\end{equation}

Unlike the Euler-Maclaurin formula (see sumem()), the Abel-Plana formula does not require derivatives. However, it only works when $|f(it)-f(-it)|$ does not increase too rapidly with $t$.

\vpara
\textbf{Examples}

The Abel-Plana formula is particularly useful when the summand decreases like a power of $k$; for example when the sum is a pure zeta function:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 25; mp.pretty = True
>>> sumap(lambda k: 1/k**2.5, [1,inf])
1.34148725725091717975677
>>> zeta(2.5)
1.34148725725091717975677
>>> sumap(lambda k: 1/(k+1j)**(2.5+2.5j), [1,inf])
(-3.385361068546473342286084 - 0.7432082105196321803869551j)
>>> zeta(2.5+2.5j, 1+1j)
(-3.385361068546473342286084 - 0.7432082105196321803869551j)
\end{lstlisting}

If the series is alternating, numerical quadrature along the real line is likely to give poor results, so it is better to evaluate the first term symbolically whenever possible:

\lstset{language={Python}}
\begin{lstlisting}
>>> n=3; z=-0.75
>>> I = expint(n,-log(z))
>>> chop(sumap(lambda k: z**k / k**n, [1,inf], integral=I))
-0.6917036036904594510141448
>>> polylog(n,z)
-0.6917036036904594510141448
\end{lstlisting}










\newpage
\section{Products}


\begin{mpFunctionsExtract}
	\mpFunctionThree
	{nprod? mpNum? a one dimensional (possibly infinite) product.}
	{f? mpNum? A one dimensional function}
	{interval? mpNum? A real interval.}	
	{Keywords? String? nsum=False.}	
\end{mpFunctionsExtract}


\vpara
Computes the product

\begin{equation}
P=\prod_{k=a}^b f(k)
\end{equation}

where = $(a,b)$ interval, and where $a=-\infty$ and/or $b=\infty$ are allowed.

By default, nprod() uses the same extrapolation methods as nsum(), except applied to the partial products rather than partial sums, and the same keyword options as for nsum() are supported. If nsum=True, the product is instead computed via nsum() as

\begin{equation}
P=\exp\left(\sum_{k=a}^b \log\left(f(k) \right) \right).
\end{equation}

This is slower, but can sometimes yield better results. It is also required (and used automatically) when Euler-Maclaurin summation is requested.

Examples

A simple finite product:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 25; mp.pretty = True
>>> nprod(lambda k: k, [1, 4])
24.0
\end{lstlisting}

A large number of infinite products have known exact values, and can therefore be used as a reference. Most of the following examples are taken from MathWorld [1].

A few infinite products with simple values are:

\lstset{language={Python}}
\begin{lstlisting}
>>> 2*nprod(lambda k: (4*k**2)/(4*k**2-1), [1, inf])
3.141592653589793238462643
>>> nprod(lambda k: (1+1/k)**2/(1+2/k), [1, inf])
2.0
>>> nprod(lambda k: (k**3-1)/(k**3+1), [2, inf])
0.6666666666666666666666667
>>> nprod(lambda k: (1-1/k**2), [2, inf])
0.5
\end{lstlisting}

Next, several more infinite products with more complicated values:

\lstset{language={Python}}
\begin{lstlisting}
>>> nprod(lambda k: exp(1/k**2), [1, inf]); exp(pi**2/6)
5.180668317897115748416626
5.180668317897115748416626
>>> nprod(lambda k: (k**2-1)/(k**2+1), [2, inf]); pi*csch(pi)
0.2720290549821331629502366
0.2720290549821331629502366
>>> nprod(lambda k: (k**4-1)/(k**4+1), [2, inf])
0.8480540493529003921296502
>>> pi*sinh(pi)/(cosh(sqrt(2)*pi)-cos(sqrt(2)*pi))
0.8480540493529003921296502
>>> nprod(lambda k: (1+1/k+1/k**2)**2/(1+2/k+3/k**2), [1, inf])
1.848936182858244485224927
>>> 3*sqrt(2)*cosh(pi*sqrt(3)/2)**2*csch(pi*sqrt(2))/pi
1.848936182858244485224927
>>> nprod(lambda k: (1-1/k**4), [2, inf]); sinh(pi)/(4*pi)
0.9190194775937444301739244
0.9190194775937444301739244
>>> nprod(lambda k: (1-1/k**6), [2, inf])
0.9826842777421925183244759
>>> (1+cosh(pi*sqrt(3)))/(12*pi**2)
0.9826842777421925183244759
>>> nprod(lambda k: (1+1/k**2), [2, inf]); sinh(pi)/(2*pi)
1.838038955187488860347849
1.838038955187488860347849
>>> nprod(lambda n: (1+1/n)**n * exp(1/(2*n)-1), [1, inf])
1.447255926890365298959138
>>> exp(1+euler/2)/sqrt(2*pi)
1.447255926890365298959138
\end{lstlisting}

The following two products are equivalent and can be evaluated in terms of a Jacobi theta function. Pi can be replaced by any value (as long as convergence is preserved):

\lstset{language={Python}}
\begin{lstlisting}
>>> nprod(lambda k: (1-pi**-k)/(1+pi**-k), [1, inf])
0.3838451207481672404778686
>>> nprod(lambda k: tanh(k*log(pi)/2), [1, inf])
0.3838451207481672404778686
>>> jtheta(4,0,1/pi)
0.3838451207481672404778686
\end{lstlisting}

This product does not have a known closed form value:

\lstset{language={Python}}
\begin{lstlisting}
>>> nprod(lambda k: (1-1/2**k), [1, inf])
0.2887880950866024212788997
\end{lstlisting}

A product taken from $-\infty$:

\lstset{language={Python}}
\begin{lstlisting}
>>> nprod(lambda k: 1-k**(-3), [-inf,-2])
0.8093965973662901095786805
>>> cosh(pi*sqrt(3)/2)/(3*pi)
0.8093965973662901095786805
\end{lstlisting}

A doubly infinite product:

\lstset{language={Python}}
\begin{lstlisting}
>>> nprod(lambda k: exp(1/(1+k**2)), [-inf, inf])
23.41432688231864337420035
>>> exp(pi/tanh(pi))
23.41432688231864337420035
\end{lstlisting}

A product requiring the use of Euler-Maclaurin summation to compute an accurate value:

\lstset{language={Python}}
\begin{lstlisting}
>>> nprod(lambda k: (1-1/k**2.5), [2, inf], method="e")
0.696155111336231052898125
\end{lstlisting}





\newpage
\section{Limits}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{limit? mpNum? an estimate of the limit.}
	{f? mpNum? A one dimensional function}
	{interval? mpNum? A real interval.}	
	{Keywords? String? direction=1, exp=False.}	
\end{mpFunctionsExtract}


\vpara
Computes an estimate of the limit

\begin{equation}
\lim\limits_{t \rightarrow x}{f(t)}
\end{equation}

where $x$ may be finite or infinite.

For finite $x$, limit() evaluates $f(x+d/n$ for consecutive integer values of $n$, where the approach direction $d$ may be specified using the direction keyword argument. For infinite $x$, limit() evaluates values of $f(\text{sign}(x) \cdot n)$.

If the approach to the limit is not sufficiently fast to give an accurate estimate directly, limit() attempts to find the limit using Richardson extrapolation or the Shanks transformation. You can select between these methods using the method keyword (see documentation of nsum() for more information).

\vpara
\textbf{Options}

The following options are available with essentially the same meaning as for nsum(): \textit{tol}, \textit{method}, \textit{maxterms}, \textit{steps}, \textit{verbose}.

If the option \textit{exp=True} is set, $f$ will be sampled at exponentially spaced points $n=2^1,2^2,2^3,\ldots$ instead of the linearly spaced points $1,2,3,\ldots$. This can sometimes improve the rate of convergence so that limit() may return a more
accurate answer (and faster). However, do note that this can only be used if $f$ supports fast and accurate evaluation for arguments that are extremely close to the limit point (or if infinite, very large arguments).

\vpara
\textbf{Examples}

A basic evaluation of a removable singularity:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 30; mp.pretty = True
>>> limit(lambda x: (x-sin(x))/x**3, 0)
0.166666666666666666666666666667
\end{lstlisting}

Computing the exponential function using its limit definition:

\lstset{language={Python}}
\begin{lstlisting}
>>> limit(lambda n: (1+3/n)**n, inf)
20.0855369231876677409285296546
>>> exp(3)
20.0855369231876677409285296546
\end{lstlisting}

A limit for $\pi$:

\lstset{language={Python}}
\begin{lstlisting}
>>> f = lambda n: 2**(4*n+1)*fac(n)**4/(2*n+1)/fac(2*n)**2
>>> limit(f, inf)
3.14159265358979323846264338328
\end{lstlisting}

Calculating the coefficient in Stirling's formula:

\lstset{language={Python}}
\begin{lstlisting}
>>> limit(lambda n: fac(n) / (sqrt(n)*(n/e)**n), inf)
2.50662827463100050241576528481
>>> sqrt(2*pi)
2.50662827463100050241576528481
\end{lstlisting}

Evaluating Euler's constant $\gamma$ using the limit representation

\begin{equation}
\gamma = \lim\limits_{n \rightarrow \infty}{\left[\left(\sum_{k=1}^n \frac{1}{k} \right) -\log(n)  \right]}
\end{equation}

which converges notoriously slowly):

\lstset{language={Python}}
\begin{lstlisting}
>>> f = lambda n: sum([mpf(1)/k for k in range(1,int(n)+1)]) - log(n)
>>> limit(f, inf)
0.577215664901532860606512090082
>>> +euler
0.577215664901532860606512090082
\end{lstlisting}

With default settings, the following limit converges too slowly to be evaluated accurately. Changing to exponential sampling however gives a perfect result:

\lstset{language={Python}}
\begin{lstlisting}
>>> f = lambda x: sqrt(x**3+x**2)/(sqrt(x**3)+x)
>>> limit(f, inf)
0.992831158558330281129249686491
>>> limit(f, inf, exp=True)
1.0
\end{lstlisting}



\newpage
\section{Extrapolation}
The following functions provide a direct interface to extrapolation algorithms. nsum() and limit() essentially work by calling the following functions with an increasing number of terms until the extrapolated limit is accurate enough.

The following functions may be useful to call directly if the precise number of terms needed to achieve a desired accuracy is known in advance, or if one wishes to study the convergence properties of the algorithms.



\subsection{Richardson's algorithm}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{richardson? mpNum? the $N$-term Richardson extrapolate for the limit.}
	{seq? mpNum? a list of the first $N$ elements of a slowly convergent infinite sequence.}	
\end{mpFunctionsExtract}


\vpara
Given a list seq of the first $N$ elements of a slowly convergent infinite sequence, richardson() computes the $N$-term Richardson extrapolate for the limit.

richardson() returns $(v,c)$ where $v$ is the estimated limit and $c$ is the magnitude of the largest weight used during the computation. The weight provides an estimate of the precision lost to cancellation. Due to cancellation effects, the sequence must be typically be computed at a much higher precision than the target accuracy of the extrapolation.

\vpara
\textbf{Applicability and issues}

The $N$-step Richardson extrapolation algorithm used by richardson() is described in [1].

Richardson extrapolation only works for a specific type of sequence, namely one converging like partial sums of $P(1)/Q(1) + P(2)/Q(2) + \dots$ where $P$ and $Q$ are polynomials. When the sequence does not convergence at such a rate richardson() generally produces garbage.

\vpara
Richardson extrapolation has the advantage of being fast: the $N$-term extrapolate requires only $O(N)$ arithmetic operations, and usually produces an estimate that is accurate to $O(N)$ digits. Contrast with the Shanks transformation (see shanks()), which requires $O(N^2)$ operations.

\vpara
richardson() is unable to produce an estimate for the approximation error. One way to estimate the error is to perform two extrapolations with slightly different $N$ and comparing the results.


Richardson extrapolation does not work for oscillating sequences. As a simple workaround, richardson() detects if the last three elements do not differ monotonically, and in that case applies extrapolation only to the even-index elements.

\vpara
\textbf{Example}

Applying Richardson extrapolation to the Leibniz series for $\pi$:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 30; mp.pretty = True
>>> S = [4*sum(mpf(-1)**n/(2*n+1) for n in range(m))
... for m in range(1,30)]
>>> v, c = richardson(S[:10])
>>> v
3.2126984126984126984126984127
>>> nprint([v-pi, c])
[0.0711058, 2.0]
>>> v, c = richardson(S[:30])
>>> v
3.14159265468624052829954206226
>>> nprint([v-pi, c])
[1.09645e-9, 20833.3]
\end{lstlisting}




\subsection{Shanks' algorithm}

\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{shanks? mpNum? the $N$-term Shanks extrapolate for the limit.}
	{seq? mpNum? a list of the first $N$ elements of a slowly convergent infinite sequence.}	
	{Keywords? String? table=None, randomized=False.}	
\end{mpFunctionsExtract}


\vpara
Given a list seq of the first $N$ elements of a slowly convergent infinite sequence $(A_k)$, shanks() computes the iterated Shanks transformation $S(A),S(S(A)),\ldots,S^{N/2}(A)$. The Shanks transformation often provides strong convergence acceleration, especially if the sequence is oscillating.

\vpara
The iterated Shanks transformation is computed using the Wynn epsilon algorithm (see [1]). shanks() returns the full epsilon table generated by Wynn's algorithm, which can be read off as follows:

\vpara
The table is a list of lists forming a lower triangular matrix, where higher row and column indices correspond to more accurate values.

The columns with even index hold dummy entries (required for the computation) and the columns with odd index hold the actual extrapolates.

The last element in the last row is typically the most accurate estimate of the limit.

The difference to the third last element in the last row provides an estimate of the approximation error.

The magnitude of the second last element provides an estimate of the numerical accuracy lost to cancellation.

\vpara
For convenience, so the extrapolation is stopped at an odd index so that shanks(seq) [-1][-1] always gives an estimate of the limit.

\vpara
Optionally, an existing table can be passed to shanks(). This can be used to efficiently extend a previous computation after new elements have been appended to the sequence. The table will then be updated in-place.

\vpara
\textbf{The Shanks transformation}

The Shanks transformation is defined as follows (see [2]): given the input sequence $(A_0,A_1,\ldots)$, the transformed sequence is given by

\begin{equation}
S(A_k) = \frac{A_{k+1} A_{k-1} - A_k^2}{A_{k+1} + A_{k-1} - 2A_k}
\end{equation}

The Shanks transformation gives the exact limit $A_{\infty}$ in a single step if $A_k=A + aq^k$. Note in particular that it extrapolates the exact sum of a geometric series in a single step.

Applying the Shanks transformation once often improves convergence substantially for an arbitrary sequence, but the optimal effect is obtained by applying it iteratively: $S(S(A_k)),S(S(S(A_k))),\ldots$.

\vpara
Wynnâ€™s epsilon algorithm provides an efficient way to generate the table of iterated Shanks transformations. It reduces the computation of each element to essentially a single division, at the cost of requiring dummy elements in the table. See [1] for details.

\vpara
\textbf{Precision issues}

Due to cancellation effects, the sequence must be typically be computed at a much higher precision than the target accuracy of the extrapolation.

If the Shanks transformation converges to the exact limit (such as if the sequence is a geometric series), then a division by zero occurs. By default, shanks() handles this case by terminating the iteration and returning the table it has generated so far. With \textit{randomized=True}, it will instead replace the zero by a pseudorandom number close to
zero. (TODO: find a better solution to this problem.)

\vpara
\textbf{Examples}

We illustrate by applying Shanks transformation to the Leibniz series for $\pi$:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 50
>>> S = [4*sum(mpf(-1)**n/(2*n+1) for n in range(m))
... for m in range(1,30)]
>>>
>>> T = shanks(S[:7])
>>> for row in T:
... nprint(row)
...
[-0.75]
[1.25, 3.16667]
[-1.75, 3.13333, -28.75]
[2.25, 3.14524, 82.25, 3.14234]
[-2.75, 3.13968, -177.75, 3.14139, -969.937]
[3.25, 3.14271, 327.25, 3.14166, 3515.06, 3.14161]
\end{lstlisting}

The extrapolated accuracy is about 4 digits, and about 4 digits may have been lost due to cancellation:

\lstset{language={Python}}
\begin{lstlisting}
>>> L = T[-1]
>>> nprint([abs(L[-1] - pi), abs(L[-1] - L[-3]), abs(L[-2])])
[2.22532e-5, 4.78309e-5, 3515.06]
\end{lstlisting}

Now we extend the computation:

\lstset{language={Python}}
\begin{lstlisting}
>>> T = shanks(S[:25], T)
>>> L = T[-1]
>>> nprint([abs(L[-1] - pi), abs(L[-1] - L[-3]), abs(L[-2])])
[3.75527e-19, 1.48478e-19, 2.96014e+17]
\end{lstlisting}

The value for pi is now accurate to 18 digits. About 18 digits may also have been lost to cancellation.

\vpara
Here is an example with a geometric series, where the convergence is immediate (the sum is exactly 1):

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> for row in shanks([0.5, 0.75, 0.875, 0.9375, 0.96875]):
... nprint(row)
[4.0]
[8.0, 1.0]
\end{lstlisting}




\subsection{Levin's algorithm}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{levin? Object? an object with sn update method. Only for use within Python}
	{Keywords? String? method='levin', variant='u'.}	
\end{mpFunctionsExtract}


\vpara
This interface implements Levin's (nonlinear) sequence transformation for convergence acceleration and summation of divergent series. It performs better than the Shanks/Wynn-epsilon algorithm for logarithmic convergent or alternating divergent series.

Let $A$ be the series we want to sum:

\begin{equation}
A=\sum_{k=0}^{\infty} a_k.
\end{equation}

Attention: all $a_k$ must be non-zero!

Let $s_n$ be the partial sums of this series:

\begin{equation}
s_n = \sum_{k=0}^n a_k.
\end{equation}

\vpara
\textbf{Methods}

Calling levin returns an object with the following methods.

update(...) works with the list of individual terms $a_k$ of $A$, and update\_step(...) works with the list of partial sums $s_k$ of $A$:

\lstset{language={Python}}
\begin{lstlisting}
v, e = ...update([a_0, a_1,..., a_k])
v, e = ...update_psum([s_0, s_1,..., s_k])
\end{lstlisting}

step(...) works with the individual terms and step\_psum(...) works with the partial sums $s_k$:

\lstset{language={Python}}
\begin{lstlisting}
v, e = ...step(a_k)
v, e = ...step_psum(s_k)
\end{lstlisting}

v is the current estimate for A, and e is an error estimate which is simply the difference between the current estimate and the last estimate. One should not mix update, update\_psum, step and step\_psum.

\vpara
\textbf{A word of caution}

One can only hope for good results (i.e. convergence acceleration or resummation) if the $s_n$ have some well defined asymptotic behavior for large $n$ and are not erratic or random. Furthermore one usually needs very high working precision because of the numerical cancellation. 

If the working precision is insufficient, levin may produce silently numerical garbage. Furthermore even if the Levin-transformation converges, in the general case there is no proof that the result is mathematically sound. Only for very special classes of problems one can prove that the Levin-transformation converges to the expected result (for example Stieltjes-type integrals). 

Furthermore the Levintransform is quite expensive (i.e. slow) in comparison to Shanks/Wynn-epsilon, Richardson \& co. In summary one can say that the Levin-transformation is powerful but unreliable and that it may need a copious amount of working precision.

\vpara
The Levin transform has several variants differing in the choice of weights. Some variants are better suited for the possible flavours of convergence behaviour of $A$ than other variants:

\lstset{language={Python}}
\begin{lstlisting}
convergence behaviour   levin-u   levin-t   levin-v   shanks/wynn-epsilon

logarithmic               +         -         +           -
linear                    +         +         +           +
alternating divergent     +         +         +           +

"+" means the variant is suitable,"-" means the variant is not suitable;
for comparison the Shanks/Wynn-epsilon transform is listed, too.
\end{lstlisting}

The variant is controlled though the variant keyword (i.e. variant="u", variant="t" or variant="v"). Overall "u" is probably the best choice.
Finally it is possible to use the Sidi-S transform instead of the Levin transform by using the keyword method='sidi'. The Sidi-S transform works better than the Levin transformation for some divergent series (see the examples).

Parameters:

\lstset{language={Python}}
\begin{lstlisting}
method      "levin" or "sidi" chooses either the Levin or the Sidi-S transformation
variant     "u","t" or "v" chooses the weight variant.
\end{lstlisting}

The Levin transform is also accessible through the nsum interface. method="l" or method="levin" select the normal Levin transform while method="sidi" selects the Sidi-S transform. The variant is in both cases selected through the levin\_variant keyword. The stepsize in nsum() must not be chosen too large, otherwise it will miss the point where the Levin transform converges resulting in numerical overflow/garbage.
For highly divergent series a copious amount of working precision must be chosen.

\vpara
\textbf{Examples}

First we sum the zeta function:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import mp
>>> mp.prec = 53
>>> eps = mp.mpf(mp.eps)
>>> with mp.extraprec(2 * mp.prec): # levin needs a high working precision
...     L = mp.levin(method = "levin", variant = "u")
...     S, s, n = [], 0, 1
...     while 1:
...         s += mp.one / (n * n)
...         n += 1
...         S.append(s)
...         v, e = L.update_psum(S)
...         if e < eps:
...             break
...         if n > 1000: raise RuntimeError("iteration limit exceeded")
>>> print(mp.chop(v - mp.pi ** 2 / 6))
0.0
>>> w = mp.nsum(lambda n: 1 / (n*n), [1, mp.inf], method = "levin", levin_variant = "u")
>>> print(mp.chop(v - w))
0.0
\end{lstlisting}

Now we sum the zeta function outside its range of convergence (attention: This does not work at the negative integers!):

\lstset{language={Python}}
\begin{lstlisting}
>>> eps = mp.mpf(mp.eps)
>>> with mp.extraprec(2 * mp.prec): # levin needs a high working precision
...     L = mp.levin(method = "levin", variant = "v")
...     A, n = [], 1
...     while 1:
...         s = mp.mpf(n) ** (2 + 3j)
...         n += 1
...         A.append(s)
...         v, e = L.update(A)
...         if e < eps:
...             break
...         if n > 1000: raise RuntimeError("iteration limit exceeded")
>>> print(mp.chop(v - mp.zeta(-2-3j)))
0.0
>>> w = mp.nsum(lambda n: n ** (2 + 3j), [1, mp.inf], method = "levin", levin_variant = "v")
>>> print(mp.chop(v - w))
0.0
\end{lstlisting}

Now we sum the divergent asymptotic expansion of an integral related to the exponential integral (see also [2] p.373). The Sidi-S transform works best here:

\lstset{language={Python}}
\begin{lstlisting}
>>> z = mp.mpf(10)
>>> exact = mp.quad(lambda x: mp.exp(-x)/(1+x/z),[0,mp.inf])
>>> # exact = z * mp.exp(z) * mp.expint(1,z) # this is the symbolic expression for the integral
>>> eps = mp.mpf(mp.eps)
>>> with mp.extraprec(2 * mp.prec): # high working precisions are mandatory for divergent resummation
...     L = mp.levin(method = "sidi", variant = "t")
...     n = 0
...     while 1:
...         s = (-1)**n * mp.fac(n) * z ** (-n)
...         v, e = L.step(s)
...         n += 1
...         if e < eps:
...             break
...         if n > 1000: raise RuntimeError("iteration limit exceeded")
>>> print(mp.chop(v - exact))
0.0
>>> w = mp.nsum(lambda n: (-1) ** n * mp.fac(n) * z ** (-n), [0, mp.inf], method = "sidi", levin_variant = "t")
>>> print(mp.chop(v - w))
0.0
\end{lstlisting}

Another highly divergent integral is also summable:

\lstset{language={Python}}
\begin{lstlisting}
>>> z = mp.mpf(2)
>>> eps = mp.mpf(mp.eps)
>>> exact = mp.quad(lambda x: mp.exp( -x * x / 2 - z * x ** 4), [0,mp.inf]) * 2 / mp.sqrt(2 * mp.pi)
>>> # exact = mp.exp(mp.one / (32 * z)) * mp.besselk(mp.one / 4, mp.one / (32 * z)) / (4 * mp.sqrt(z * mp.pi)) # this is the symbolic expression for the integral
>>> with mp.extraprec(7 * mp.prec):  # we need copious amount of precision to sum this highly divergent series
...     L = mp.levin(method = "levin", variant = "t")
...     n, s = 0, 0
...     while 1:
...         s += (-z)**n * mp.fac(4 * n) / (mp.fac(n) * mp.fac(2 * n) * (4 ** n))
...         n += 1
...         v, e = L.step_psum(s)
...         if e < eps:
...             break
...         if n > 1000: raise RuntimeError("iteration limit exceeded")
>>> print(mp.chop(v - exact))
0.0
>>> w = mp.nsum(lambda n: (-z)**n * mp.fac(4 * n) / (mp.fac(n) * mp.fac(2 * n) * (4 ** n)),
...   [0, mp.inf], method = "levin", levin_variant = "t", workprec = 8*mp.prec, steps = [2] + [1 for x in xrange(1000)])
>>> print(mp.chop(v - w))
0.0
\end{lstlisting}

These examples run with 15-20 decimal digits precision. For higher precision the working precision must be raised.

\vpara
\textbf{Examples for nsum}

Here we calculate Euler's constant as the constant term in the Laurent expansion of $\zeta(s)$ at $s=1$. This sum converges extremly slowly because of the logarithmic convergence behaviour of the Dirichlet series for zeta:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 30
>>> z = mp.mpf(10) ** (-10)
>>> a = mp.nsum(lambda n: n**(-(1+z)), [1, mp.inf], method = "l") - 1 / z
>>> print(mp.chop(a - mp.euler, tol = 1e-10))
0.0
\end{lstlisting}

The Sidi-S transform performs excellently for the alternating series of $\log(2)$:

\lstset{language={Python}}
\begin{lstlisting}
>>> a = mp.nsum(lambda n: (-1)**(n-1) / n, [1, mp.inf], method = "sidi")
>>> print(mp.chop(a - mp.log(2)))
0.0
\end{lstlisting}

Hypergeometric series can also be summed outside their range of convergence. The stepsize in nsum() must not be chosen too large, otherwise it will miss the point where the Levin transform converges resulting in numerical overflow/garbage:

\lstset{language={Python}}
\begin{lstlisting}
>>> z = 2 + 1j
>>> exact = mp.hyp2f1(2 / mp.mpf(3), 4 / mp.mpf(3), 1 / mp.mpf(3), z)
>>> f = lambda n: mp.rf(2 / mp.mpf(3), n) * mp.rf(4 / mp.mpf(3), n) * z**n / (mp.rf(1 / mp.mpf(3), n) * mp.fac(n))
>>> v = mp.nsum(f, [0, mp.inf], method = "levin", steps = [10 for x in xrange(1000)])
>>> print(mp.chop(exact-v))
0.0
\end{lstlisting}



\subsection{Cohen's algorithm}

\begin{mpFunctionsExtract}
	\mpFunctionZero
	{cohen\_alt? Object? an object with sn update method. Only for use within Python}	
\end{mpFunctionsExtract}


\vpara
This interface implements the convergence acceleration of alternating series as described in H. Cohen, F.R. Villegas, D. Zagier - "Convergence Acceleration of Alternating Series". This series transformation works only well if the individual terms of the series have an alternating sign. It belongs to the class of linear series transformations (in contrast to the Shanks/Wynn-epsilon or Levin transform). This series transformation is also able to sum some types of divergent series. See the paper under which conditions this resummation is mathematical sound.

Let $A$ be the series we want to sum:
\begin{equation}
A=\sum_{k=0}^{\infty} a_k
\end{equation}

Let $s_n$ be the partial sums of this series:
\begin{equation}
s_n=\sum_{k=0}^n a_k.
\end{equation}

\vpara
\textbf{Interface}

Calling cohen\_alt returns an object with the following methods.

Then update(...) works with the list of individual terms $a_k$ and update\_psum(...) works with the list of partial sums $s_k$:

\lstset{language={Python}}
\begin{lstlisting}
v, e = ...update([a_0, a_1,..., a_k])
v, e = ...update_psum([s_0, s_1,..., s_k])
\end{lstlisting}


v is the current estimate for A, and e is an error estimate which is simply the difference between the current estimate and the last estimate.

\vpara
\textbf{Examples}

Here we compute the alternating zeta function using update\_psum:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import mp
>>> AC = mp.cohen_alt()
>>> S, s, n = [], 0, 1
>>> while 1:
... s += -((-1) ** n) * mp.one / (n * n)
... n += 1
... S.append(s)
... v, e = AC.update_psum(S)
... if e < mp.eps:
... break
... if n > 1000: raise RuntimeError("iteration limit exceeded")
>>> print(mp.chop(v - mp.pi ** 2 / 12))
0.0
\end{lstlisting}

Here we compute the product $\prod_{n=1}^{\infty} \Gamma(1+1/(2n-1))/\Gamma(1+1/(2n))$:

\lstset{language={Python}}
\begin{lstlisting}
>>> A = []
>>> AC = mp.cohen_alt()
>>> n = 1
>>> while 1:
... A.append( mp.loggamma(1 + mp.one / (2 * n - 1)))
... A.append(-mp.loggamma(1 + mp.one / (2 * n)))
... n += 1
... v, e = AC.update(A)
... if e < mp.eps:
... break
... if n > 1000: raise RuntimeError("iteration limit exceeded")
>>> v = mp.exp(v)
>>> print(mp.chop(v - 1.06215090557106, tol = 1e-12))
0.0
\end{lstlisting}

cohen\_alt is also accessible through the nsum() interface:

\lstset{language={Python}}
\begin{lstlisting}
>>> v = mp.nsum(lambda n: (-1)**(n-1) / n, [1, mp.inf], method = "a")
>>> print(mp.chop(v - mp.log(2)))
0.0
>>> v = mp.nsum(lambda n: (-1)**n / (2 * n + 1), [0, mp.inf], method = "a")
>>> print(mp.chop(v - mp.pi / 4))
0.0
>>> v = mp.nsum(lambda n: (-1)**n * mp.log(n) * n, [1, mp.inf], method = "a")
>>> print(mp.chop(v - mp.diff(lambda s: mp.altzeta(s), -1)))
0.0
\end{lstlisting}





\newpage
\chapter{Differentiation}

\section{Numerical derivatives}


\subsection{One-dimensional Differentiation}

\begin{mpFunctionsExtract}
	\mpFunctionFour
	{diff? mpNum? the $n$-th derivative $f^{(n)}(x)$.}
	{f? mpNum? A one dimensional function}
	{x? mpNum? A real number.}
	{n? mpNum? An integer, indicating the nth derivative.}		
	{Keywords? String? method=step, tol=eps, direction=0. Many more, see below.}	
\end{mpFunctionsExtract}


\vpara
Numerically computes the derivative of $f$, $f'(x)$, or generally for an integer $n>0$, the $n$-th derivative $f^{(n)}(x)$. A few basic examples are:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>> diff(lambda x: x**2 + x, 1.0)
3.0
>>> diff(lambda x: x**2 + x, 1.0, 2)
2.0
>>> diff(lambda x: x**2 + x, 1.0, 3)
0.0
>>> nprint([diff(exp, 3, n) for n in range(5)])   # exp'(x) = exp(x)
[20.0855, 20.0855, 20.0855, 20.0855, 20.0855]
\end{lstlisting}

Even more generally, given a tuple of arguments $(x_1,\dots,x_k)$ and  order $(n_1,\dots,n_k)$, the partial derivative $f^{(n_1,\dots,n_k)}(x_1,\dots,x_k)$ is evaluated. For example:

\lstset{language={Python}}
\begin{lstlisting}
>>> diff(lambda x,y: 3*x*y + 2*y - x, (0.25, 0.5), (0,1))
2.75
>>> diff(lambda x,y: 3*x*y + 2*y - x, (0.25, 0.5), (1,1))
3.0
\end{lstlisting}

\vpara
\textbf{Options}

The following optional keyword arguments are recognized:

\vpara
\textit{method}: Supported methods are 'step' or 'quad': derivatives may be computed using either a finite difference with a small step size $h$ (default), or numerical quadrature.

\vpara
\textit{direction}: Direction of finite difference: can be -1 for a left difference, 0 for a central difference (default), or +1 for a right difference; more generally can be any complex number.

\vpara
\textit{addprec}: Extra precision for $h$ used to account for the functionâ€™s sensitivity to perturbations (default = 10).

\vpara
\textit{relative}: Choose $h$ relative to the magnitude of $x$, rather than an absolute value; useful for large or tiny $x$(default = False).

\vpara
\textit{h}: As an alternative to addprec and relative, manually select the step size $h$.

\vpara
\textit{singular}: If True, evaluation exactly at the point $x$ is avoided; this is useful for differentiating functions with removable singularities. Default = False.

\vpara
\textit{radius}: Radius of integration contour (with method = 'quad'). Default = 0.25. A larger radius typically is faster and more accurate, but it must be chosen so that $f$ has no singularities within the radius from the evaluation point.

\vpara
A finite difference requires $n+1$ function evaluations and must be performed at $n+1$ times the target precision. Accordingly, $f$ must support fast evaluation at high precision.

With integration, a larger number of function evaluations is required, but not much extra precision is required. For high order derivatives, this method may thus be faster if f is very expensive to evaluate at high precision.

\vpara
\textbf{Further examples}

The direction option is useful for computing left- or right-sided derivatives of nonsmooth functions:

\lstset{language={Python}}
\begin{lstlisting}
>>> diff(abs, 0, direction=0)
0.0
>>> diff(abs, 0, direction=1)
1.0
>>> diff(abs, 0, direction=-1)
-1.0
\end{lstlisting}

More generally, if the direction is nonzero, a right difference is computed where the step size is multiplied by sign(direction). For example, with direction=+j, the derivative from the positive imaginary direction will be computed:

\lstset{language={Python}}
\begin{lstlisting}
>>> diff(abs, 0, direction=j)
(0.0 - 1.0j)
\end{lstlisting}

With integration, the result may have a small imaginary part even even if the result is purely real:

\lstset{language={Python}}
\begin{lstlisting}
>>> diff(sqrt, 1, method="quad")
(0.5 - 4.59...e-26j)
>>> chop(_)
0.5
\end{lstlisting}

Adding precision to obtain an accurate value:

\lstset{language={Python}}
\begin{lstlisting}
>>> diff(cos, 1e-30)
0.0
>>> diff(cos, 1e-30, h=0.0001)
-9.99999998328279e-31
>>> diff(cos, 1e-30, addprec=100)
-1.0e-30
\end{lstlisting}



\subsection{Sequence of derivatives}

\begin{mpFunctionsExtract}
	\mpFunctionFour
	{diffs? mpNum? the $n$-th derivative $f^{(n)}(x)$.}
	{f? mpNum? A one dimensional function}
	{x? mpNum? A real number.}
	{n? mpNum? An integer, indicating the nth derivative.}		
	{Keywords? String? method=step, tol=eps, direction=0. Many more, see below.}	
\end{mpFunctionsExtract}

\vpara
Returns a generator that yields the sequence of derivatives

\begin{equation}
f(x),f'(x),f''(x),\ldots,f^{(k)(x),\ldots}
\end{equation}

With method='step', diffs() uses only $O(k)$ function evaluations to generate the first $k$ derivatives, rather than the roughly $O(k^2)$ evaluations required if one calls diff () $k$ separate times.

With $n<\infty$, the generator stops as soon as the $n$-th derivative has been generated. If the exact number of needed derivatives is known in advance, this is further slightly more efficient.

Options are the same as for diff().

\vpara
\textbf{Examples}

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15
>>> nprint(list(diffs(cos, 1, 5)))
[0.540302, -0.841471, -0.540302, 0.841471, 0.540302, -0.841471]
>>> for i, d in zip(range(6), diffs(cos, 1)):
... print("%s %s" % (i, d))
...
0 0.54030230586814
1 -0.841470984807897
2 -0.54030230586814
3 0.841470984807897
4 0.54030230586814
5 -0.841470984807897
\end{lstlisting}




\newpage
\section{Composition of derivatives}

\subsection{Differentiation of products of functions}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{diffsprod? mpNum? the result of the differentiation of products of functions.}
	{factors? Object? a list of $N$ iterables or generators.}	
\end{mpFunctionsExtract}


\vpara
Given a list of $N$ iterables or generators yielding $f_k(x),f_k'(x),f_k''(x),\ldots$ for $k=1,\ldots,N$, generate $g_k(x),g_k'(x),g_k''(x),\ldots$ where $g(x)=f_1(x)f_2(x)\cdots f_N(x)$.

At high precision and for large orders, this is typically more efficient than numerical differentiation if the derivatives of each $f_k(x)$ admit direct computation.

Note: This function does not increase the working precision internally, so guard digits may have to be added externally for full accuracy.

Examples

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>> f = lambda x: exp(x)*cos(x)*sin(x)
>>> u = diffs(f, 1)
>>> v = mp.diffs_prod([diffs(exp,1), diffs(cos,1), diffs(sin,1)])
>>> next(u); next(v)
1.23586333600241
1.23586333600241
>>> next(u); next(v)
0.104658952245596
0.104658952245596
>>> next(u); next(v)
-5.96999877552086
-5.96999877552086
>>> next(u); next(v)
-12.4632923122697
-12.4632923122697
\end{lstlisting}



\subsection{Differentiation of the exponential of functions}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{diffsexp? mpNum? the result of the differentiation of the exponential of functions.}
	{fdiffs? Object? a list of $N$ iterables or generators.}	
\end{mpFunctionsExtract}


\vpara
Given an iterable or generator yielding $f_k(x),f_k'(x),f_k''(x),\ldots$ generate $g_k(x),g_k'(x),g_k''(x),\ldots$ where $g(x)=\exp(f(x))$.

At high precision and for large orders, this is typically more efficient than numerical differentiation if the derivatives of $f(x)$ admit direct computation.

Note: This function does not increase the working precision internally, so guard digits may have to be added externally for full accuracy.

Examples

The derivatives of the gamma function can be computed using logarithmic
differentiation:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>>
>>> def diffs_loggamma(x):
... yield loggamma(x)
... i = 0
... while 1:
... yield psi(i,x)
... i += 1
...
>>> u = diffs_exp(diffs_loggamma(3))
>>> v = diffs(gamma, 3)
>>> next(u); next(v)
2.0
2.0
>>> next(u); next(v)
1.84556867019693
1.84556867019693
>>> next(u); next(v)
2.49292999190269
2.49292999190269
>>> next(u); next(v)
3.44996501352367
3.44996501352367
\end{lstlisting}



\section{Fractional derivatives}


\begin{mpFunctionsExtract}
	\mpFunctionFour
	{differint? mpNum? the Riemann-Liouville differintegral.}
	{f? mpNum? A one dimensional function}
	{x? mpNum? A real interval.}	
	{n? mpNum? A real interval.}
	{x0? mpNum? A real interval.}				
\end{mpFunctionsExtract}

\vpara
Calculates the Riemann-Liouville differintegral, or fractional derivative, defined by

\begin{equation}
{}_{x_0}\mathbb{D}_x^n f(x) \frac{1}{\Gamma(m-n)} \frac{d^m}{dx^m} \int_{x_0}^{x} (x-t)^{m-n-1} f(t)dt
\end{equation}

where $f$ is a given (presumably well-behaved) function, $x$ is the evaluation point, $n$ is the order, and $x_0$ is the reference point of integration ($m$ is an arbitrary parameter selected automatically).

\vpara
With $n=1$, this is just the standard derivative $f'(x)$; with $n=2$, the second derivative $f''(x)$, etc. With $n=-1$, it gives $\int_{x_0}^x f(t)dt$, with $n=-1$ it gives $\int_{x_0}^x \left( \int_{x_0}^x f(u)du \right) dt$, etc.

\vpara
As $n$ is permitted to be any number, this operator generalizes iterated differentiation and iterated integration to a single operator with a continuous order parameter.

\vpara
\textbf{Examples}

There is an exact formula for the fractional derivative of a monomial $x^p$, which may be used as a reference. For example, the following gives a half-derivative (order 0.5):

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>> x = mpf(3); p = 2; n = 0.5
>>> differint(lambda t: t**p, x, n)
7.81764019044672
>>> gamma(p+1)/gamma(p-n+1) * x**(p-n)
7.81764019044672
\end{lstlisting}

Another useful test function is the exponential function, whose integration / differentiation formula easy generalizes to arbitrary order. Here we first compute a third derivative, and then a triply nested integral. (The reference point $x_0$is set to $-\infty$ to avoid nonzero endpoint terms.):

\lstset{language={Python}}
\begin{lstlisting}
>>> differint(lambda x: exp(pi*x), -1.5, 3)
0.278538406900792
>>> exp(pi*-1.5) * pi**3
0.278538406900792
>>> differint(lambda x: exp(pi*x), 3.5, -3, -inf)
1922.50563031149
>>> exp(pi*3.5) / pi**3
1922.50563031149
\end{lstlisting}

However, for noninteger $n$, the differentiation formula for the exponential function must be modified to give the same result as the Riemann-Liouville differintegral:

\lstset{language={Python}}
\begin{lstlisting}
>>> x = mpf(3.5)
>>> c = pi
>>> n = 1+2*j
>>> differint(lambda x: exp(c*x), x, n)
(-123295.005390743 + 140955.117867654j)
>>> x**(-n) * exp(c)**x * (x*c)**n * gammainc(-n, 0, x*c) / gamma(-n)
(-123295.005390743 + 140955.117867654j)
\end{lstlisting}








\newpage

\chapter{Numerical integration (quadrature)}
\section{Standard quadrature}



\subsection{One-dimensional Integration}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{quad? mpNum? a one dimensional integral.}
	{f? mpNum? A one dimensional function}
	{interval? mpNum? A real interval.}	
	{Keywords? String? method=TanhSinh, error=False, verbose=False, maxdegree. Many more, see below.}	
\end{mpFunctionsExtract}

\vpara
Computes a single, double or triple integral over a given 1D interval, 2D rectangle, or 3D cuboid. A basic example:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>> quad(sin, [0, pi])
2.0
\end{lstlisting}

A basic 2D integral:

\lstset{language={Python}}
\begin{lstlisting}
>>> f = lambda x, y: cos(x+y/2)
>>> quad2d(f, [-pi/2, pi/2], [0, pi])
4.0
\end{lstlisting}

\vpara
\textbf{Interval format}

The integration range for each dimension may be specified using a list or tuple. Arguments are interpreted as follows:

\vpara
quad(f, [x1, x2]) calculates $\int_{x_1}^{x_2} f(x)\: dx$

\vpara
quad2d(f, [x1, x2], [y1, y2])  calculates $\int_{x_1}^{x_2}  \int_{y_1}^{y_2} f(x,y) \: dy \: dx$


\vpara
quad3d(f, [x1, x2], [y1, y2], [z1, z2])  calculates $\int_{x_1}^{x_2}  \int_{y_1}^{y_2} \int_{z_1}^{z_2} f(x,y,z)\: dz \: dy \: dx$

\vpara
Endpoints may be finite or infinite. An interval descriptor may also contain more than two points. In this case, the integration is split into subintervals, between each pair of consecutive points. This is useful for dealing with mid-interval discontinuities, or
integrating over large intervals where the function is irregular or oscillates.

\vpara
\textbf{Options}

quad() recognizes the following keyword arguments:

\vpara
\textit{method}: Chooses integration algorithm (described below).

\vpara
\textit{error}: If set to true, quad() returns $(v,e)$ where $v$ is the integral and $e$ is the estimated error.

\vpara
\textit{maxdegree}: Maximum degree of the quadrature rule to try before quitting.

\vpara
\textit{verbose}: Print details about progress.

\vpara
\textbf{Algorithms}

Mpmath presently implements two integration algorithms: tanh-sinh quadrature and Gauss-Legendre quadrature. These can be selected using method='tanh-sinh' or method='gauss-legendre' or by passing the classes method=TanhSinh, method=GaussLegendre. The functions quadts() and quadgl() are also available as shortcuts.

\vpara
Both algorithms have the property that doubling the number of evaluation points roughly doubles the accuracy, so both are ideal for high precision quadrature (hundreds or thousands of digits).

\vpara
At high precision, computing the nodes and weights for the integration can be expensive (more expensive than computing the function values). To make repeated integrations fast, nodes are automatically cached.

\vpara
The advantages of the tanh-sinh algorithm are that it tends to handle endpoint singularities well, and that the nodes are cheap to compute on the first run. For these reasons, it is used by quad() as the default algorithm.

\vpara
Gauss-Legendre quadrature often requires fewer function evaluations, and is therefore often faster for repeated use, but the algorithm does not handle endpoint singularities as well and the nodes are more expensive to compute. Gauss-Legendre quadrature can be a better choice if the integrand is smooth and repeated integrations are required
(e.g. for multiple integrals).

\vpara
See the documentation for TanhSinh and GaussLegendre for additional details. 

\vpara
\textbf{Examples of 1D integrals}

Intervals may be infinite or half-infinite. The following two examples evaluate the limits of the inverse tangent function $(\int 1/(1+x^2)=\tan^{-1} x)$, and the Gaussian integral $\int_{-\infty}^{\infty}\exp(-x^2)\: dx=\sqrt{\pi}$:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> quad(lambda x: 2/(x**2+1), [0, inf])
3.14159265358979
>>> quad(lambda x: exp(-x**2), [-inf, inf])**2
3.14159265358979
\end{lstlisting}


Integrals can typically be resolved to high precision. The following computes 50 digits of $\pi$ by integrating the area of the half-circle defined by $x^2+y^2 \leq 1, -1<x<1, y \geq 0$:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 50
>>> 2*quad(lambda x: sqrt(1-x**2), [-1, 1])
3.1415926535897932384626433832795028841971693993751
\end{lstlisting}

One can just as well compute 1000 digits (output truncated):

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 1000
>>> 2*quad(lambda x: sqrt(1-x**2), [-1, 1])
3.141592653589793238462643383279502884...216420198
\end{lstlisting}

Complex integrals are supported. The following computes a residue at $z=0$ by integrating counterclockwise along the diamond-shaped path from $1$ to $+i$ to $-1$ to $-i$ to $1$:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> chop(quad(lambda z: 1/z, [1,j,-1,-j,1]))
(0.0 + 6.28318530717959j)
\end{lstlisting}

%\vpara
%\textbf{Examples of 2D and 3D integrals}


\subsubsection{Singularities}

Both tanh-sinh and Gauss-Legendre quadrature are designed to integrate smooth (infinitely differentiable) functions. Neither algorithm copes well with mid-interval singularities (such as mid-interval discontinuities in $f(x)$ or $f'(x)$). The best solution is to split the integral into parts:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> quad(lambda x: abs(sin(x)), [0, 2*pi]) # Bad
3.99900894176779
>>> quad(lambda x: abs(sin(x)), [0, pi, 2*pi]) # Good
4.0
\end{lstlisting}

The tanh-sinh rule often works well for integrands having a singularity at one or both endpoints:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> quad(log, [0, 1], method="tanh-sinh") # Good
-1.0
>>> quad(log, [0, 1], method="gauss-legendre") # Bad
-0.999932197413801
\end{lstlisting}

However, the result may still be inaccurate for some functions:

\lstset{language={Python}}
\begin{lstlisting}
>>> quad(lambda x: 1/sqrt(x), [0, 1], method="tanh-sinh")
1.99999999946942
\end{lstlisting}

This problem is not due to the quadrature rule per se, but to numerical amplification of errors in the nodes. The problem can be circumvented by temporarily increasing the precision:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 30
>>> a = quad(lambda x: 1/sqrt(x), [0, 1], method="tanh-sinh")
>>> mp.dps = 15
>>> +a
2.0
\end{lstlisting}



\subsubsection{Highly variable functions}

For functions that are smooth (in the sense of being infinitely differentiable) but contain sharp mid-interval peaks or many 'bumps', quad() may fail to provide full accuracy.

For example, with default settings, quad() is able to integrate $\sin(x)$ accurately over an interval of length 100 but not over length 1000:

\lstset{language={Python}}
\begin{lstlisting}
>>> quad(sin, [0, 100]); 1-cos(100) # Good
0.137681127712316
0.137681127712316
>>> quad(sin, [0, 1000]); 1-cos(1000) # Bad
-37.8587612408485
0.437620923709297
\end{lstlisting}

One solution is to break the integration into 10 intervals of length 100:

\lstset{language={Python}}
\begin{lstlisting}
>>> quad(sin, linspace(0, 1000, 10)) # Good
0.437620923709297
\end{lstlisting}

Another is to increase the degree of the quadrature:

\lstset{language={Python}}
\begin{lstlisting}
>>> quad(sin, [0, 1000], maxdegree=10) # Also good
0.437620923709297
\end{lstlisting}

Whether splitting the interval or increasing the degree is more efficient differs from case to case. Another example is the function $1/(1+x^2)$, which has a sharp peak centered around $x=0$:

\lstset{language={Python}}
\begin{lstlisting}
>>> f = lambda x: 1/(1+x**2)
>>> quad(f, [-100, 100]) # Bad
3.64804647105268
>>> quad(f, [-100, 100], maxdegree=10) # Good
3.12159332021646
>>> quad(f, [-100, 0, 100]) # Also good
3.12159332021646
\end{lstlisting}



\subsection{Two-dimensional Integration}

\begin{mpFunctionsExtract}
	\mpFunctionFour
	{quad2d? mpNum? a two dimensional integral.}
	{f? mpNum? A one dimensional function}
	{interval1? mpNum? A real interval.}	
	{interval2? mpNum? A real interval.}		
	{Keywords? String? method=TanhSinh, error=False, verbose=False, maxdegree. Many more, see below.}	
\end{mpFunctionsExtract}

\vpara
Here are several nice examples of analytically solvable 2D integrals (taken from MathWorld [1]) that can be evaluated to high precision fairly rapidly by quad():

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 30
>>> f = lambda x, y: (x-1)/((1-x*y)*log(x*y))
>>> quad2d(f, [0, 1], [0, 1])
0.577215664901532860606512090082
>>> +euler
0.577215664901532860606512090082
>>> f = lambda x, y: 1/sqrt(1+x**2+y**2)
>>> quad2d(f, [-1, 1], [-1, 1])
3.17343648530607134219175646705
>>> 4*log(2+sqrt(3))-2*pi/3
3.17343648530607134219175646705
>>> f = lambda x, y: 1/(1-x**2 * y**2)
>>> quad2d(f, [0, 1], [0, 1])
1.23370055013616982735431137498
>>> pi**2 / 8
1.23370055013616982735431137498
>>> quad2d(lambda x, y: 1/(1-x*y), [0, 1], [0, 1])
1.64493406684822643647241516665
>>> pi**2 / 6
1.64493406684822643647241516665
\end{lstlisting}

Multiple integrals may be done over infinite ranges:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> print(quad2d(lambda x,y: exp(-x-y), [0, inf], [1, inf]))
0.367879441171442
>>> print(1/e)
0.367879441171442
\end{lstlisting}

For nonrectangular areas, one can call quad() recursively. For example, we can replicate the earlier example of calculating $\pi$ by integrating over the unit-circle, and use double quadrature to actually measure the area circle:

\lstset{language={Python}}
\begin{lstlisting}
>>> f = lambda x: quad(lambda y: 1, [-sqrt(1-x**2), sqrt(1-x**2)]
>>> quad(f, [-1, 1])
3.14159265358979
\end{lstlisting}



\subsection{Three-dimensional Integration}

\begin{mpFunctionsExtract}
	\mpFunctionFive
	{quad3d? mpNum? a three dimensional integral.}
	{f? mpNum? A one dimensional function}
	{interval1? mpNum? A real interval.}	
	{interval2? mpNum? A real interval.}		
	{interval3? mpNum? A real interval.}	
	{Keywords? String? method=TanhSinh, error=False, verbose=False, maxdegree. Many more, see below.}	
\end{mpFunctionsExtract}

\vpara
Here is a simple triple integral:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> f = lambda x,y,z: x*y/(1+z)
>>> quad3d(f, [0,1], [0,1], [1,2], method="gauss-legendre")
0.101366277027041
>>> (log(3)-log(2))/4
0.101366277027041
\end{lstlisting}




\subsection{Oscillatory integrals}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{quadosc? mpNum? a one dimensional oscillatory integral.}
	{f? mpNum? A one dimensional function}
	{interval? mpNum? A real interval.}	
	{Keywords? String? omega=None, period=None, zeros=None}	
\end{mpFunctionsExtract}

\vpara
Calculates

\begin{equation}
I=\int_a^b f(x) dx
\end{equation}

where at least one of $a$ and $b$ is infinite and where $f(x) = g(x) \cos(\omega x + \phi)$ for some slowly decreasing function $g(x)$. With proper input, quadosc() can also handle oscillatory integrals where the oscillation rate is different from a pure sine or cosine wave.

\vpara
In the standard case when $|a|<\infty, b=\infty$, quadosc() works by evaluating the infinite series

\begin{equation}
I=\int_a^{x_1} f(x) dx + \sum_{k=1}^{\infty} \int_{x_k}^{x_{k+1}} f(x) dx
\end{equation}

where $x_k$ are consecutive zeros (alternatively some other periodic reference point) of $f(x)$. Accordingly, quadosc() requires information about the zeros of $f(x)$. For a periodic function, you can specify the zeros by either providing the angular frequency $\omega$ (omega) or the period $2\pi/\omega$. In general, you can specify the $n$-th zero by providing the zeros arguments. Below is an example of each:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>> f = lambda x: sin(3*x)/(x**2+1)
>>> quadosc(f, [0,inf], omega=3)
0.37833007080198
>>> quadosc(f, [0,inf], period=2*pi/3)
0.37833007080198
>>> quadosc(f, [0,inf], zeros=lambda n: pi*n/3)
0.37833007080198
>>> (ei(3)*exp(-3)-exp(3)*ei(-3))/2 # Computed by Mathematica
0.37833007080198
\end{lstlisting}

Note that zeros was specified to multiply $n$ by the half-period, not the full period. In theory, it does not matter whether each partial integral is done over a half period or a full period. However, if done over half-periods, the infinite series passed to nsum() becomes an alternating series and this typically makes the extrapolation much more efficient.

Here is an example of an integration over the entire real line, and a half-infinite integration starting at $-\infty$:

\lstset{language={Python}}
\begin{lstlisting}
>>> quadosc(lambda x: cos(x)/(1+x**2), [-inf, inf], omega=1)
1.15572734979092
>>> pi/e
1.15572734979092
>>> quadosc(lambda x: cos(x)/x**2, [-inf, -1], period=2*pi)
-0.0844109505595739
>>> cos(1)+si(1)-pi/2
-0.0844109505595738
\end{lstlisting}

Of course, the integrand may contain a complex exponential just as well as a real sine or cosine:

\lstset{language={Python}}
\begin{lstlisting}
>>> quadosc(lambda x: exp(3*j*x)/(1+x**2), [-inf,inf], omega=3)
(0.156410688228254 + 0.0j)
>>> pi/e**3
0.156410688228254
>>> quadosc(lambda x: exp(3*j*x)/(2+x+x**2), [-inf,inf], omega=3)
(0.00317486988463794 - 0.0447701735209082j)
>>> 2*pi/sqrt(7)/exp(3*(j+sqrt(7))/2)
(0.00317486988463794 - 0.0447701735209082j)
\end{lstlisting}



\subsubsection{Non-periodic functions}

If $f(x)=g(x) h(x)$ for some function $h(x)$ that is not strictly periodic, omega or period might not work, and it might be necessary to use zeros.

A notable exception can be made for Bessel functions which, though not periodic, are 'asymptotically periodic' in a sufficiently strong sense that the sum extrapolation will work out:

\lstset{language={Python}}
\begin{lstlisting}
>>> quadosc(j0, [0, inf], period=2*pi)
1.0
>>> quadosc(j1, [0, inf], period=2*pi)
1.0
\end{lstlisting}

More properly, one should provide the exact Bessel function zeros:

\lstset{language={Python}}
\begin{lstlisting}
>>> j0zero = lambda n: findroot(j0, pi*(n-0.25))
>>> quadosc(j0, [0, inf], zeros=j0zero)
1.0
\end{lstlisting}

For an example where zeros becomes necessary, consider the complete Fresnel integrals

\begin{equation}
\int_0^{\infty} \cos x^2 dx = \int_0^{\infty} \sin x^2 dx = \sqrt{\frac{\pi}{8}}
\end{equation}

Although the integrands do not decrease in magnitude as $x \rightarrow \infty$, the integrals are convergent since the oscillation rate increases (causing consecutive periods to asymptotically cancel out). These integrals are virtually impossible to calculate to any kind of accuracy using standard quadrature rules. However, if one provides the correct asymptotic distribution of zeros ($x_n \sim \sqrt{n}$), quadosc() works:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 30
>>> f = lambda x: cos(x**2)
>>> quadosc(f, [0,inf], zeros=lambda n:sqrt(pi*n))
0.626657068657750125603941321203
>>> f = lambda x: sin(x**2)
>>> quadosc(f, [0,inf], zeros=lambda n:sqrt(pi*n))
0.626657068657750125603941321203
>>> sqrt(pi/8)
0.626657068657750125603941321203
\end{lstlisting}

(Interestingly, these integrals can still be evaluated if one places some other constant than $\pi$ in the square root sign.)

In general, if $f(x) \sim g(x) \cos(h(x))$, the zeros follow the inverse-function distribution $h^{-1}(x)$:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> f = lambda x: sin(exp(x))
>>> quadosc(f, [1,inf], zeros=lambda n: log(n))
-0.25024394235267
>>> pi/2-si(e)
-0.250243942352671
\end{lstlisting}



\subsubsection{Non-alternating functions}

If the integrand oscillates around a positive value, without alternating signs, the extrapolation might fail. A simple trick that sometimes works is to multiply or divide the frequency by 2:

\lstset{language={Python}}
\begin{lstlisting}
>>> f = lambda x: 1/x**2+sin(x)/x**4
>>> quadosc(f, [1,inf], omega=1) # Bad
1.28642190869861
>>> quadosc(f, [1,inf], omega=0.5) # Perfect
1.28652953559617
>>> 1+(cos(1)+ci(1)+sin(1))/6
1.28652953559617
\end{lstlisting}

\vpara
\textbf{Fast decay}

quadosc() is primarily useful for slowly decaying integrands. If the integrand decreases exponentially or faster, quad() will likely handle it without trouble (and generally be much faster than quadosc()):

\lstset{language={Python}}
\begin{lstlisting}
>>> quadosc(lambda x: cos(x)/exp(x), [0, inf], omega=1)
0.5
>>> quad(lambda x: cos(x)/exp(x), [0, inf])
0.5
\end{lstlisting}


\newpage
\section{Main Quadrature rules}
%
%Quadrature rules are implemented using this class, in order to simplify the code and provide a common infrastructure for tasks such as error estimation and node caching.
%
%You can implement a custom quadrature rule by subclassing QuadratureRule and implementing the appropriate methods. The subclass can then be used by quad() by passing it as the method argument.
%
%QuadratureRule instances are supposed to be singletons. QuadratureRule
%therefore implements instance caching in \_\_new\_\_().
%
%\vpara
%calc\_nodes(degree, prec, verbose=False)
%
%Compute nodes for the standard interval $[-1,1]$. Subclasses should probably implement only this method, and use get\_nodes() method to retrieve the nodes.
%
%\vpara
%clear()
%
%Delete cached node data.
%
%\vpara
%estimate\_error(results, prec, epsilon)
%
%Given results from integrations $[I_1,I2,\ldots,I_k]$ done with a quadrature of rule of degree $1,2,\ldots,k$, estimate the error of $I_k$.
%
%\vpara
%For $k=2$, we estimate $|I_{\infty} - I_2|$ as $|I_2-I_1|$.
%
%\vpara
%For $k>2$, we extrapolate $|I_{\infty} - I_k| \approx |I_{k+1} - I_k|$from $|I_k-I_{k-1}|$ and $|I_k-I_{k-2}|$ under the assumption that each degree increment roughly doubles the accuracy of the quadrature rule (this is true for both TanhSinh and GaussLegendre). The extrapolation formula is given by Borwein, Bailey \& Girgensohn. Although not very conservative, this method seems to be very robust in practice.
%
%\vpara
%get\_nodes(a, b, degree, prec, verbose=False)
%
%Return nodes for given interval, degree and precision. The nodes are retrieved from a cache if already computed; otherwise they are computed by calling calc\_nodes () and are then cached.
%Subclasses should probably not implement this method, but just implement
%calc\_nodes() for the actual node computation.
%
%
%\vpara
%guess\_degree(prec)
%
%Given a desired precision $p$ in bits, estimate the degree $m$ of the quadrature required to accomplish full accuracy for typical integrals. By default, quad() will perform up to $m$ iterations. The value of should be a slight overestimate, so that 'slightly bad' integrals can be dealt with automatically using a few extra iterations.
%
%On the other hand, it should not be too big, so quad() can quit within a reasonable amount of time when it is given an 'unsolvable' integral.
%
%\vpara
%The default formula used by guess\_degree() is tuned for both TanhSinh and GaussLegendre. The output is roughly as follows:
%
%50: 6
%
%100: 7
%
%500: 10
%
%3000: 12
%
%This formula is based purely on a limited amount of experimentation and will sometimes be wrong.
%
%\vpara
%sum\_next(f, nodes, degree, prec, previous, verbose=False)
%
%Evaluates the step sum $\sum w_kf(x_k)$ where the nodes list contains the
%$(w_k,x_k)$ pairs.
%
%summation() will supply the list results of values computed by sum\_next() at previous degrees, in case the quadrature rule is able to reuse them.
%
%
%\vpara
%summation(f, points, prec, epsilon, max\_degree, verbose=False)
%Main integration function. Computes the 1D integral over the interval specified by points. For each subinterval, performs quadrature of degree from 1 up to max\_degree until estimate\_error() signals convergence.
%
%summation() transforms each subintegration to the standard interval and then calls sum\_next().
%
%\vpara
%transform\_nodes(nodes, a, b, verbose=False)

%Rescale standardized nodes (for $[-1,1]$) to a general interval $[a,b]$.

For a finite interval, a simple linear change of variables is used. Otherwise, the following transformations are used:

\begin{equation}
[a,\infty] : t=\frac{1}{x} + (a-1)
\end{equation}
\begin{equation}
[-\infty,b] : t= (b+1) -\frac{1}{x} 
\end{equation}
\begin{equation}
[-\infty,\infty] : t=\frac{x}{\sqrt{1-x^2}} 
\end{equation}


\subsection{TanhSinh}
\label{DoubleExponentialTransformation}

Tanh-sinh quadrature is a method for numerical integration introduced by Hidetosi Takahasi and Masatake Mori in 1974. It uses the change of variables
\begin{equation}
x = \tanh \left(\tfrac{1}{2} \pi \sinh t \right)
\end{equation}

to transform an integral on the interval $x \in (âˆ’1, +1)$ to an integral on the entire real line $t âˆˆ (-\infty,\infty)$. After this transformation, the integrand decays with a double exponential rate, and thus, this method is also known as the double exponential (DE) formula.

For a given step size $h$, the integral is approximated by the sum
\begin{equation}
\int_{-1}^1 f(x) dx \approx \sum_{k=-\infty}^\infty w_k f(x_k),
\end{equation}

with the abscissas
\begin{equation}
x_k = \tanh \left(\tfrac{1}{2} \pi \sinh t_k \right)
\end{equation}

and the weights
\begin{equation}
w_k = \frac{\tanh \tfrac{1}{2} \pi \cosh t_k}{\cosh^2 (\tfrac{1}{2} \pi \sinh t_k)}
\end{equation}

where $t_k=t_0+hk$ for a step length $h \sim 2^{-m}$.

Like Gaussian quadrature, Tanh-Sinh quadrature is well suited for arbitrary-precision integration, where an accuracy of hundreds or even thousands of digits is desired. The convergence is exponential (in the discretization sense) for sufficiently well-behaved integrands: doubling the number of evaluation points roughly doubles the number of correct digits.

Tanh-Sinh quadrature is less efficient than Gaussian quadrature for smooth integrands, but unlike Gaussian quadrature tends to work equally well with integrands having singularities or infinite derivatives at one or both endpoints of the integration interval. A further advantage is that the abscissas and weights are relatively easy to compute. The cost of calculating abscissa-weight pairs for n-digit accuracy is roughly $n^2 \log^2 n$ compared to $n^3 \log n$ for Gaussian quadrature.



\vpara
This class implements 'tanh-sinh' or 'doubly exponential' quadrature. This quadrature rule is based on the Euler-Maclaurin integral formula. By performing a change of variables involving nested exponentials / hyperbolic functions (hence the name), the derivatives at the endpoints vanish rapidly. Since the error term in the Euler-Maclaurin formula depends on the derivatives at the endpoints, a simple step sum becomes
extremely accurate. In practice, this means that doubling the number of evaluation points roughly doubles the number of accurate digits.


\vpara
The implementation of the tanh-sinh algorithm is based on the description given in Borwein, Bailey \& Girgensohn, "Experimentation in Mathematics - Computational Paths to Discovery", A K Peters, 2003, pages 312-313. In the present implementation, a few improvements have been made:

\vpara
A more efficient scheme is used to compute nodes (exploiting recurrence for the exponential function). The nodes are computed successively instead of all at once.


We exploit the fact that half of the abscissas at degree are precisely the abscissas from degree . Thus reusing the result from the previous level allows a 2x speedup.



A summary is given by \cite{Bailey_2006,Bailey_2004}.

See also the external links at \href{http://en.wikipedia.org/wiki/Tanh-sinh_quadrature}{Wikipedia}.
%
%
%\section{Finite Intervals}
%The procedures intdeo and intdei use the Double Exponential (DE) transformation (developed by M. Mori, T. Ooura, and others) for automatic quadrature of f(x) over the infinite interval (a,+INF) for functions with and without oscillatory factors resp. The DE transformation is very powerful if the abscissas a+xi can be processed with high accuracy; therefore the two routines are best used with a=0, otherwise there can be severe roundoff problems (a+xi=a).
%
%
%\section{Infinite Intervals}
%The procedures intdeo and intdei use the Double Exponential (DE) transformation (developed by M. Mori, T. Ooura, and others) for automatic quadrature of f(x) over the infinite interval (a,+INF) for functions with and without oscillatory factors resp. The DE transformation is very powerful if the abscissas a+xi can be processed with high accuracy; therefore the two routines are best used with a=0, otherwise there can be severe roundoff problems (a+xi=a).
%
%





\subsection{Gauss-Legendre}
\label{GaussLegendreGenerell}

This class implements Gauss-Legendre quadrature, which is exceptionally efficient for polynomials and polynomial-like (i.e. very smooth) integrands.

The abscissas and weights are given by roots and values of Legendre polynomials, which are the orthogonal polynomials on $[-1,1]$ with respect to the unit weight (see legendre()).

In this implementation, we take the 'degree' of the quadrature to denote a Gauss-Legendre rule of degree $3 \cdot 2^m$ (following Borwein, Bailey \& Girgensohn). This way we get quadratic, rather than linear, convergence as the degree is incremented.

Comparison to tanh-sinh quadrature:

Is faster for smooth integrands once nodes have been computed

Initial computation of nodes is usually slower

Handles endpoint singularities worse

Handles infinite integration intervals worse

%
%
%calc\_nodes(degree, prec, verbose=False)
%
%Calculates the abscissas and weights for Gauss-Legendre quadrature of degree of given degree (actually $3 \cdot 2^m$).

\newpage
\section{Additional Quadrature rules}


\subsection{Gauss-Legendre}
\begin{equation}
\int_a^b f(x) dx = \frac{b-a}{2} \sum_{i=1}^n w_i f\left(\tfrac{1}{2}(b-a)(x_i +1)\right) + R_n, \quad \text{where}
\end{equation}
\begin{equation}
w_i = \frac{2}{((1-x^2)P_n' (x_i))^2}, \quad R_n=\frac{f^{(2n)}(x)(b-a)^{2n+1}(n!)^4}{((2n+1)(2n)!)^3} \text{ for } a<x<b,
\end{equation}
$P_n$ are the Legendre polynomials, and $x_i$ is the $i$th zero of $P_n$.




\subsection{Gauss-Hermite}
\begin{equation}
\int_{-\infty}^\infty e^{-ax^2} f(x) dx = \frac{1}{\sqrt{a}} \sum_{i=1}^n w_i f\left(\frac{x_i}{\sqrt{a}}\right) + R_n, \quad \text{where}
\end{equation}
\begin{equation}
w_i = \frac{2^{n-1} n! \sqrt{\pi}}{(nH_{n-1} (x_i))^2}, \quad R_n=\frac{f^{(2n)}(x) n! \sqrt{\pi}}{2^n (2n)!} \text{ for } -\infty<x<\infty,
\end{equation}
$H_n$ are the Hermite polynomials, and $x_i$ is the $i$th zero of $H_n$.





\subsection{Gauss-Laguerre}
\begin{equation}
\int_0^\infty e^{-ax} f(x) dx = \frac{1}{a} \sum_{i=1}^n w_i f\left(\frac{x_i}{a}\right) + R_n, \quad \text{where}
\end{equation}
\begin{equation}
w_i = \frac{x_i}{((n+1)L_{n+1} (x_i))^2}, \quad R_n=\frac{f^{(2n)}(x) (n!)^2}{(2n)!} \text{ for } 0<x<\infty,
\end{equation}
$L_n$ are the Laguerre polynomials, and $x_i$ is the $i$th zero of $L_n$.






\newpage
\chapter{Ordinary differential equations}

\section{Solving the ODE initial value problem}


\begin{mpFunctionsExtract}
	\mpFunctionFour
	{odefun? mpNum? a function $y(x) = [y_0(x),y_1(x),\ldots,y_n(x)]$ that is a numerical solution of a $n+1$-dimensional first-order ordinary differential equation (ODE) system}
	{f? mpNum? A one dimensional function}
	{x0? mpNum? A real number.}	
	{y0? mpNum? A real number.}		
	{Keywords? String?  tol=None, degree=None, method='taylor', verbose=False}	
\end{mpFunctionsExtract}


%\subsection{odefun(ctx, F, x0, y0, tol=None, degree=None, method='taylor', verbose=False)}

The (ODE) system has the form

\begin{equation}
y'_0(x) = F_0(x,[y_0(x),y_1(x),\ldots,y_n(x)])
\end{equation}
\begin{equation}
y'_0(x) = F_1(x,[y_0(x),y_1(x),\ldots,y_n(x)])
\end{equation}

\begin{equation}
y'_0(x) = F_n(x,[y_0(x),y_1(x),\ldots,y_n(x)])
\end{equation}


The derivatives are specified by the vector-valued function $F$ that evaluates 

$[y'_0,\ldots,y'_n] = F(x,[y_0,\ldots,y_n])$. 

The initial point $x_0$ is specified by the scalar argument x0, and the initial value 

$y(x_0)=[y_0(x_0,\ldots,y_n(x_0)]$ 

is specified by the vector argument y0.

\vpara
For convenience, if the system is one-dimensional, you may optionally provide just a scalar value for y0. In this case, $F$ should accept a scalar y argument and return a scalar. The solution function y will return scalar values instead of length-1 vectors.

\vpara
Evaluation of the solution function $y(x)$ is permitted for any $x>x_0$.

\vpara
A high-order ODE can be solved by transforming it into first-order vector form. This transformation is described in standard texts on ODEs. Examples will also be given below.

\vpara
\textbf{Options, speed and accuracy}

By default, odefun() uses a high-order Taylor series method. For reasonably wellbehaved problems, the solution will be fully accurate to within the working precision. Note that F must be possible to evaluate to very high precision for the generation of Taylor series to work.

\vpara
To get a faster but less accurate solution, you can set a large value for tol (which defaults roughly to eps). If you just want to plot the solution or perform a basic simulation, tol = 0.01 is likely sufficient.

\vpara
The degree argument controls the degree of the solver (with method=â€™taylorâ€™, this is the degree of the Taylor series expansion). A higher degree means that a longer step can be taken before a new local solution must be generated from F, meaning that fewer steps are required to get from $x_0$ to a given $x_1$. On the other hand, a higher degree also means that each local solution becomes more expensive (i.e., more evaluations of F are required per step, and at higher precision).

\vpara
The optimal setting therefore involves a tradeoff. Generally, decreasing the degree for Taylor series is likely to give faster solution at low precision, while increasing is likely to be better at higher precision.

\vpara
The function object returned by odefun() caches the solutions at all step points and uses polynomial interpolation between step points. Therefore, once $y(x_1)$ has been evaluated for some $x_1$, $y(x)$ can be evaluated very quickly for any $x_0 \leq x \leq x_1$, and continuing the evaluation up to $x_2>x_1$ is also fast.

\vpara
\textbf{Examples of first-order ODEs}

We will solve the standard test problem $y'(x) = y(x), y(0)=1$ which has explicit solution $y(x)=\exp(x)$:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>> f = odefun(lambda x, y: y, 0, 1)
>>> for x in [0, 1, 2.5]:
... print((f(x), exp(x)))
...
(1.0, 1.0)
(2.71828182845905, 2.71828182845905)
(12.1824939607035, 12.1824939607035)
\end{lstlisting}

The solution with high precision:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 50
>>> f = odefun(lambda x, y: y, 0, 1)
>>> f(1)
2.7182818284590452353602874713526624977572470937
>>> exp(1)
2.7182818284590452353602874713526624977572470937
\end{lstlisting}

Using the more general vectorized form, the test problem can be input as (note that f returns a 1-element vector):

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> f = odefun(lambda x, y: [y[0]], 0, [1])
>>> f(1)
[2.71828182845905]
\end{lstlisting}

odefun() can solve nonlinear ODEs, which are generally impossible (and at best difficult) to solve analytically. As an example of a nonlinear ODE, we will solve $y'(x)=x \sin(y(x))$ for $y(0)=\pi/2$. An exact solution happens to be known for this problem, and is given by $y(x)=2 \tan^{-1}(\exp(x^2/2))$:

\lstset{language={Python}}
\begin{lstlisting}
>>> f = odefun(lambda x, y: x*sin(y), 0, pi/2)
>>> for x in [2, 5, 10]:
... print((f(x), 2*atan(exp(mpf(x)**2/2))))
...
(2.87255666284091, 2.87255666284091)
(3.14158520028345, 3.14158520028345)
(3.14159265358979, 3.14159265358979)
\end{lstlisting}

If $F$ is independent of $y$, an ODE can be solved using direct integration. We can therefore obtain a reference solution with quad():

\lstset{language={Python}}
\begin{lstlisting}
>>> f = lambda x: (1+x**2)/(1+x**3)
>>> g = odefun(lambda x, y: f(x), pi, 0)
>>> g(2*pi)
0.72128263801696
>>> quad(f, [pi, 2*pi])
0.72128263801696
\end{lstlisting}


\vpara
\textbf{Examples of second-order ODEs}

We will solve the harmonic oscillator equation $y''(x)+y(x)=0$. To do this, we introduce the helper functions $y_0=y, y_1=y'_0$ whereby the original equation can be written as $y'_1+y'_0=0$. Put together, we get the first-order, two-dimensional vector ODE

\begin{equation}
y'_0 = y_1; \quad y'_1 = -y_0.
\end{equation}


To get a well-defined IVP, we need two initial values. With $y(0)=y_0(0)=1$ and $-y'(0)=y_1(0)=0$, the problem will of course be solved by $y(x)=y_0(x)=\cos(x)$ and $y(x)=y_0(x)=\sin(x)$. We check this:

\lstset{language={Python}}
\begin{lstlisting}
>>> f = odefun(lambda x, y: [-y[1], y[0]], 0, [1, 0])
>>> for x in [0, 1, 2.5, 10]:
... nprint(f(x), 15)
... nprint([cos(x), sin(x)], 15)
... print("---")
...
[1.0, 0.0]
[1.0, 0.0]
---
[0.54030230586814, 0.841470984807897]
[0.54030230586814, 0.841470984807897]
---
[-0.801143615546934, 0.598472144103957]
[-0.801143615546934, 0.598472144103957]
---
[-0.839071529076452, -0.54402111088937]
[-0.839071529076452, -0.54402111088937]
---
\end{lstlisting}

Note that we get both the sine and the cosine solutions simultaneously.








\newpage
\chapter{Function approximation}

\section{Taylor series}


\begin{mpFunctionsExtract}
	\mpFunctionFour
	{taylor? mpNum? a list of coefficients of a degree-$n$ Taylor polynomial around the point $x$ of the given function $f$.}
	{f? mpNum? A one dimensional function}
	{x? mpNum? A real number.}	
	{n? mpNum? A real number.}		
	{Keywords? String?  method=step, tol=eps, direction=0. Many more, see diff()}	
\end{mpFunctionsExtract}


\vpara
Examples:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>> nprint(chop(taylor(sin, 0, 5)))
[0.0, 1.0, 0.0, -0.166667, 0.0, 0.00833333]
\end{lstlisting}

The coefficients are computed using high-order numerical differentiation. The function must be possible to evaluate to arbitrary precision. See diff() for additional details and supported keyword options.

Note that to evaluate the Taylor polynomial as an approximation of $f$, e.g. with polyval(), the coefficients must be reversed, and the point of the Taylor expansion must be subtracted from the argument:

\lstset{language={Python}}
\begin{lstlisting}
>>> p = taylor(exp, 2.0, 10)
>>> polyval(p[::-1], 2.5 - 2.0)
12.1824939606092
>>> exp(2.5)
12.1824939607035
\end{lstlisting}



\newpage
\section{Pade approximation}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{pade? mpNum? coefficients of a Pade approximation of degree $(L,M)$ to a function}
	{a? mpNum? A list of at least $L+M+1$ Taylor coefficients approximating a function $A(x)$ }
	{L? mpNum? An integer, specifying the degree of polynomials $P$.}	
	{M? mpNum? An integer, specifying the degree of polynomials $Q$.}			
\end{mpFunctionsExtract}


\vpara
Computes a Pade approximation of degree $(L,M)$ to a function. Given at least $L+M+1$ Taylor coefficients approximating a function $A(x)$, pade() returns coefficients of polynomials $P,Q$ satisfying

\begin{equation}
P=\sum_{k=0}^L p_kx^k; \quad Q=\sum_{k=0}^M q_kx^k; \quad Q_0=1; \quad A(x)Q(x)=P(x)+O(x^{L+M+1})
\end{equation}

$P(x)/Q(x)$ can provide a good approximation to an analytic function beyond the radius of convergence of its Taylor series (example from G.A. Baker 'Essentials of Pade Approximants' Academic Press, Ch.1A):

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>> one = mpf(1)
>>> def f(x):
... return sqrt((one + 2*x)/(one + x))
...
>>> a = taylor(f, 0, 6)
>>> p, q = pade(a, 3, 3)
>>> x = 10
>>> polyval(p[::-1], x)/polyval(q[::-1], x)
1.38169105566806
>>> f(x)
1.38169855941551
\end{lstlisting}




\newpage
\section{Chebyshev approximation}


\begin{mpFunctionsExtract}
	\mpFunctionFour
	{chebyfit? mpNum? coefficients of a polynomial of degree $N-1$ that approximates the given function $f$ on the interval $[a,b]$}
	{f? mpNum? A one dimensional function}
	{interval? mpNum? A real interval.}	
	{N? mpNum? An integer.}		
	{Keywords? String? error=False}	
\end{mpFunctionsExtract}

\vpara
Computes a polynomial of degree $N-1$ that approximates the given function $f$ on the interval $[a,b]$. With error=True, chebyfit() also returns an accurate estimate of the maximum absolute error; that is, the maximum value of $|f(x) - P(x)|$ for $x \in [a,b]$.

\vpara
chebyfit() uses the Chebyshev approximation formula, which gives a nearly optimal solution: that is, the maximum error of the approximating polynomial is very close to the smallest possible for any polynomial of the same degree.

\vpara
Chebyshev approximation is very useful if one needs repeated evaluation of an expensive function, such as function defined implicitly by an integral or a differential equation. (For example, it could be used to turn a slow mpFormulaPy function into a fast machine-precision version of the same.)

\vpara
\textbf{Examples}

Here we use chebyfit() to generate a low-degree approximation of $f(x) = \cos(x)$, valid on the interval $[1,2]$:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>> poly, err = chebyfit(cos, [1, 2], 5, error=True)
>>> nprint(poly)
[0.00291682, 0.146166, -0.732491, 0.174141, 0.949553]
>>> nprint(err, 12)
1.61351758081e-5
\end{lstlisting}

The polynomial can be evaluated using polyval:

\lstset{language={Python}}
\begin{lstlisting}
>>> nprint(polyval(poly, 1.6), 12)
-0.0291858904138
>>> nprint(cos(1.6), 12)
-0.0291995223013
\end{lstlisting}

Sampling the true error at 1000 points shows that the error estimate generated by chebyfit is remarkably good:

\lstset{language={Python}}
\begin{lstlisting}
>>> error = lambda x: abs(cos(x) - polyval(poly, x))
>>> nprint(max([error(1+n/1000.) for n in range(1000)]), 12)
1.61349954245e-5
\end{lstlisting}

\vpara
\textbf{Choice of degree}

The degree $N$ can be set arbitrarily high, to obtain an arbitrarily good approximation. As a rule of thumb, an $N$-term Chebyshev approximation is good to $N/(b-a)$ decimal places on a unit interval (although this depends on how well-behaved $f$ is). The cost grows accordingly: chebyfit evaluates the function $(N^2)/2$ times to compute the coefficients and an additional $N$ times to estimate the error.

\vpara
\textbf{Possible issues}

One should be careful to use a sufficiently high working precision both when calling chebyfit and when evaluating the resulting polynomial, as the polynomial is sometimes ill-conditioned. It is for example difficult to reach 15-digit accuracy when evaluating the polynomial using machine precision floats, no matter the theoretical accuracy of the polynomial. (The option to return the coefficients in Chebyshev form should be made available in the future.)

It is important to note the Chebyshev approximation works poorly if $f$ is not smooth. A function containing singularities, rapid oscillation, etc can be approximated more effectively by multiplying it by a weight function that cancels out the nonsmooth features, or by dividing the interval into several segments.


\newpage
\section{Fourier series}


\begin{mpFunctionsExtract}
	\mpFunctionThree
	{fourier? mpNum? two lists of coefficients of the Fourier series of degree $N$ of the given function on the interval $[a,b]$..}
	{f? mpNum? A one dimensional function}
	{interval? mpNum? A real interval.}	
	{N? mpNum? An integer.}		
\end{mpFunctionsExtract}

\vpara
Computes the Fourier series of degree $N$ of the given function on the interval $[a,b]$. More precisely, fourier() returns two lists $(c,s)$ of coefficients (the cosine series and sine series, respectively), such that

\begin{equation}
f(x) \sim \sum_{k=0}^N c_k \cos(kmx) + s_k \sin(kmx)
\end{equation}

where $m=2\pi/(b-a)$.

Note that many texts define the first coefficient as $2c_0$ instead of $c_0$. The easiest way to evaluate the computed series correctly is to pass it to fourierval().

\vpara
\textbf{Examples}

The function $f(x)=x$ has a simple Fourier series on the standard interval $[-\pi,\pi]$. The cosine coefficients are all zero (because the function has odd symmetry), and the sine coefficients are rational numbers:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>> c, s = fourier(lambda x: x, [-pi, pi], 5)
>>> nprint(c)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
>>> nprint(s)
[0.0, 2.0, -1.0, 0.666667, -0.5, 0.4]
\end{lstlisting}

This computes a Fourier series of a nonsymmetric function on a nonstandard interval:

\lstset{language={Python}}
\begin{lstlisting}
>>> I = [-1, 1.5]
>>> f = lambda x: x**2 - 4*x + 1
>>> cs = fourier(f, I, 4)
>>> nprint(cs[0])
[0.583333, 1.12479, -1.27552, 0.904708, -0.441296]
>>> nprint(cs[1])
[0.0, -2.6255, 0.580905, 0.219974, -0.540057]
\end{lstlisting}

It is instructive to plot a function along with its truncated Fourier series:

\lstset{language={Python}}
\begin{lstlisting}
>>> plot([f, lambda x: fourierval(cs, I, x)], I)
\end{lstlisting}

Fourier series generally converge slowly (and may not converge pointwise). For example, if $f(x) = \cosh(x)$, a 10-term Fourier series gives an $L^2$ error corresponding to 2-digit accuracy:

\lstset{language={Python}}
\begin{lstlisting}
>>> I = [-1, 1]
>>> cs = fourier(cosh, I, 9)
>>> g = lambda x: (cosh(x) - fourierval(cs, I, x))**2
>>> nprint(sqrt(quad(g, I)))
0.00467963
\end{lstlisting}

fourier() uses numerical quadrature. For nonsmooth functions, the accuracy (and speed) can be improved by including all singular points in the interval specification:

\lstset{language={Python}}
\begin{lstlisting}
>>> nprint(fourier(abs, [-1, 1], 0), 10)
([0.5000441648], [0.0])
>>> nprint(fourier(abs, [-1, 0, 1], 0), 10)
([0.5], [0.0])
\end{lstlisting}


\vspace{0.6cm}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{fouriereval? mpNum? the result of the evaluation of a Fourier series (in the format computed by by fourier() for the given interval) at the point $x$.}
	{series? mpNum? a pair $(c,s)$ where $c$ is the cosine series and $s$ is the sine series}
	{interval? mpNum? A real interval.}	
	{x? mpNum? A real number.}		
\end{mpFunctionsExtract}


\vpara
The series should be a pair $(c,s)$ where $c$ is the cosine series and $s$ is the sine series. The two lists need not have the same length.






\chapter{Number identification}

Most functions in mpFormulaPy are concerned with producing approximations from exact mathematical formulas. It is also useful to consider the inverse problem: given only a decimal approximation for a number, such as 0.7320508075688772935274463, is it possible to find an exact formula?

Subject to certain restrictions, such 'reverse engineering' is indeed possible thanks to the existence of integer relation algorithms. Mpmath implements the PSLQ algorithm (developed by H. Ferguson), which is one such algorithm.

Automated number recognition based on PSLQ is not a silver bullet. Any occurring transcendental constants ($\pi$, $e$, etc) must be guessed by the user, and the relation between those constants in the formula must be linear (such as $x=3\pi+4e$). More complex formulas can be found by combining PSLQ with functional transformations; however, this is only feasible to a limited extent since the computation time grows exponentially with the number of operations that need to be combined.

The number identification facilities in mpFormulaPy are inspired by the Inverse Symbolic Calculator (ISC). The ISC is more powerful than mpFormulaPy, as it uses a lookup table of millions of precomputed constants (thereby mitigating the problem with exponential complexity).



\section{Constant recognition}




\begin{mpFunctionsExtract}
	\mpFunctionThree
	{identify? String? the result of an attempt to find an exact formula for a given real number $x$}
	{x? mpNum? A real number}
	{constants? String? A list of known constants.}	
	{Keywords? String?  tol=None, maxcoeff=1000, full=False, verbose=False}	
\end{mpFunctionsExtract}


\vpara
Given a real number $x$, identify(x) attempts to find an exact formula for $x$. This formula is returned as a string. If no match is found, None is returned. With full=True, a list of matching formulas is returned.

\vpara
As a simple example, identify() will find an algebraic formula for the golden ratio:


\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>> identify(phi)
'((1+sqrt(5))/2)'
\end{lstlisting}


identify() can identify simple algebraic numbers and simple combinations of given base constants, as well as certain basic transformations thereof. More specifically, identify() looks for the following:

1. Fractions

2. Quadratic algebraic numbers

3. Rational linear combinations of the base constants

4. Any of the above after first transforming $x$ into $f(x)$ where $f(x)$is $1/x$, $\sqrt{x}$, $x^2$, or $\exp(x)$, either directly or with $x$ or $f(x)$ multiplied or divided by one of the base constants 

5. Products of fractional powers of the base constants and small integers

\vpara
Base constants can be given as a list of strings representing mpFormulaPy expressions (identify() will eval the strings to numerical values and use the original strings for the output), or as a dict of formula:value pairs.

\vpara
In order not to produce spurious results, identify() should be used with high precision; preferably 50 digits or more.



\subsection{Examples}

Simple identifications can be performed safely at standard precision. Here the default recognition of rational, algebraic, and exp/log of algebraic numbers is demonstrated:


\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> identify(0.22222222222222222)
'(2/9)'
>>> identify(1.9662210973805663)
'sqrt(((24+sqrt(48))/8))'
>>> identify(4.1132503787829275)
'exp((sqrt(8)/2))'
>>> identify(0.881373587019543)
'log(((2+sqrt(8))/2))'
\end{lstlisting}


By default, identify() does not recognize $\pi$. At standard precision it finds a not too useful approximation. At slightly increased precision, this approximation is no longer accurate enough and identify() more correctly returns None:

\lstset{language={Python}}
\begin{lstlisting}
>>> identify(pi)
'(2**(176/117)*3**(20/117)*5**(35/39))/(7**(92/117))'
>>> mp.dps = 30
>>> identify(pi)
>>>
\end{lstlisting}


Numbers such as $\pi$, and simple combinations of user-defined constants, can be identified if they are provided explicitly:

\lstset{language={Python}}
\begin{lstlisting}
>>> identify(3*pi-2*e, ['pi', 'e'])
'(3*pi + (-2)*e)'
\end{lstlisting}


Here is an example using a dict of constants. Note that the constants need not be 'atomic'; identify() can just as well express the given number in terms of expressions given by formulas:

\lstset{language={Python}}
\begin{lstlisting}
>>> identify(pi+e, {'a':pi+2, 'b':2*e})
'((-2) + 1*a + (1/2)*b)'
\end{lstlisting}


Next, we attempt some identifications with a set of base constants. It is necessary to increase the precision a bit.

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 50
>>> base = ['sqrt(2)','pi','log(2)']
>>> identify(0.25, base)
'(1/4)'
>>> identify(3*pi + 2*sqrt(2) + 5*log(2)/7, base)
'(2*sqrt(2) + 3*pi + (5/7)*log(2))'
>>> identify(exp(pi+2), base)
'exp((2 + 1*pi))'
>>> identify(1/(3+sqrt(2)), base)
'((3/7) + (-1/7)*sqrt(2))'
>>> identify(sqrt(2)/(3*pi+4), base)
'sqrt(2)/(4 + 3*pi)'
>>> identify(5**(mpf(1)/3)*pi*log(2)**2, base)
'5**(1/3)*pi*log(2)**2'
\end{lstlisting}


An example of an erroneous solution being found when too low precision is used:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> identify(1/(3*pi-4*e+sqrt(8)), ['pi', 'e', 'sqrt(2)'])
'((11/25) + (-158/75)*pi + (76/75)*e + (44/15)*sqrt(2))'
>>> mp.dps = 50
>>> identify(1/(3*pi-4*e+sqrt(8)), ['pi', 'e', 'sqrt(2)'])
'1/(3*pi + (-4)*e + 2*sqrt(2))'
\end{lstlisting}



\subsection{Finding approximate solutions}

The tolerance tol defaults to 3/4 of the working precision. Lowering the tolerance is useful for finding approximate matches. We can for example try to generate approximations for pi:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> identify(pi, tol=1e-2)
'(22/7)'
>>> identify(pi, tol=1e-3)
'(355/113)'
>>> identify(pi, tol=1e-10)
'(5**(339/269))/(2**(64/269)*3**(13/269)*7**(92/269))'
\end{lstlisting}


With full=True, and by supplying a few base constants, identify can generate almost endless lists of approximations for any number (the output below has been truncated to show only the first few):

\lstset{language={Python}}
\begin{lstlisting}
>>> for p in identify(pi, ['e', 'catalan'], tol=1e-5, full=True):
... print(p)
...
e/log((6 + (-4/3)*e))
(3**3*5*e*catalan**2)/(2*7**2)
sqrt(((-13) + 1*e + 22*catalan))
log(((-6) + 24*e + 4*catalan)/e)
exp(catalan*((-1/5) + (8/15)*e))
catalan*(6 + (-6)*e + 15*catalan)
sqrt((5 + 26*e + (-3)*catalan))/e
e*sqrt(((-27) + 2*e + 25*catalan))
log(((-1) + (-11)*e + 59*catalan))
((3/20) + (21/20)*e + (3/20)*catalan)
...
\end{lstlisting}


The numerical values are roughly as close to $\pi$ as permitted by the specified tolerance:

\lstset{language={Python}}
\begin{lstlisting}
>>> e/log(6-4*e/3)
3.14157719846001
>>> 135*e*catalan**2/98
3.14166950419369
>>> sqrt(e-13+22*catalan)
3.14158000062992
>>> log(24*e-6+4*catalan)-1
3.14158791577159
\end{lstlisting}



\subsection{Symbolic processing}

The output formula can be evaluated as a Python expression. Note however that if fractions (like '2/3') are present in the formula, Python's eval() may erroneously perform integer division. Note also that the output is not necessarily in the algebraically simplest
form:

\lstset{language={Python}}
\begin{lstlisting}
>>> identify(sqrt(2))
'(sqrt(8)/2)'
\end{lstlisting}

As a solution to both problems, consider using SymPy's sympify() to convert the formula into a symbolic expression. SymPy can be used to pretty-print or further simplify the formula symbolically:

\lstset{language={Python}}
\begin{lstlisting}
>>> from sympy import sympify
>>> sympify(identify(sqrt(2)))
2**(1/2)
\end{lstlisting}


Sometimes identify() can simplify an expression further than a symbolic algorithm:

\lstset{language={Python}}
\begin{lstlisting}
>>> from sympy import simplify
>>> x = sympify('-1/(-3/2+(1/2)*5**(1/2))*(3/2-1/2*5**(1/2))**(1/2)')
>>> x
(3/2 - 5**(1/2)/2)**(-1/2)
>>> x = simplify(x)
>>> x
2/(6 - 2*5**(1/2))**(1/2)
>>> mp.dps = 30
>>> x = sympify(identify(x.evalf(30)))
>>> x
1/2 + 5**(1/2)/2
\end{lstlisting}


(In fact, this functionality is available directly in SymPy as the function nsimplify(), which is essentially a wrapper for identify().)


\subsection{Miscellaneous issues and limitations}

The input $x$ must be a real number. All base constants must be positive real numbers and must not be rationals or rational linear combinations of each other.

The worst-case computation time grows quickly with the number of base constants. Already with 3 or 4 base constants, identify() may require several seconds to finish. To search for relations among a large number of constants, you should consider using pslq() directly.

The extended transformations are applied to x, not the constants separately. As a result, identify will for example be able to recognize exp(2*pi+3) with pi given as a base constant, but not 2*exp(pi)+3. It will be able to recognize the latter if exp(pi) is given explicitly as a base constant.


\newpage
\section{Algebraic identification}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{findpoly? String? the coefficients of an integer polynomial $P$ of degree at most $n$ such that $P(x) \sim 0$}
	{x? mpNum? A real number}
	{n? Integer? max degree of polynomial.}	
	{Keywords? String?  tol=None, maxcoeff=1000, maxsteps=100, verbose=False}	
\end{mpFunctionsExtract}


\vpara
findpoly(x, n) returns the coefficients of an integer polynomial $P$ of degree at most $n$ such that $P(x) \sim 0$. If no polynomial having $x$ as a root can be found, findpoly() returns None.

\vpara
findpoly() works by successively calling pslq() with the vectors $[1,x]$, $[1,x,x^2]$, $[1,x,x^2,x^3]$, ..., as input. Keyword arguments given to findpoly() are forwarded verbatim to pslq(). In particular, you can specify a tolerance for $P(x)$ with tol and a maximum permitted coefficient size with maxcoeff.

For large values of $n$, it is recommended to run findpoly() at high precision; preferably 50 digits or more.


\subsection{Examples}

By default (degree $n=1$), findpoly() simply finds a linear polynomial with a rational root:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>> findpoly(0.7)
[-10, 7]
\end{lstlisting}


The generated coefficient list is valid input to polyval and polyroots:

\lstset{language={Python}}
\begin{lstlisting}
>>> nprint(polyval(findpoly(phi, 2), phi), 1)
-2.0e-16
>>> for r in polyroots(findpoly(phi, 2)):
... print(r)
...
-0.618033988749895
1.61803398874989
\end{lstlisting}


Numbers of the form $m+n\sqrt{p}$ for integers $(m,n,p)$ are solutions to quadratic equations. As we find here, $1+\sqrt{2}$ is a root of the polynomial $x^2-2x-1$:

\lstset{language={Python}}
\begin{lstlisting}
>>> findpoly(1+sqrt(2), 2)
[1, -2, -1]
>>> findroot(lambda x: x**2 - 2*x - 1, 1)
2.4142135623731
\end{lstlisting}


Despite only containing square roots, the following number results in a polynomial of degree 4:

\lstset{language={Python}}
\begin{lstlisting}
>>> findpoly(sqrt(2)+sqrt(3), 4)
[1, 0, -10, 0, 1]
\end{lstlisting}


In fact, $x^4-10x^2+1$ is the minimal polynomial of $r=\sqrt{2}+\sqrt{3}$, meaning that a rational polynomial of lower degree having $r$ as a root does not exist. Given sufficient precision, findpoly() will usually find the correct minimal polynomial of a given algebraic number.



\subsection{Non-algebraic numbers}

If findpoly() fails to find a polynomial with given coefficient size and tolerance constraints, that means no such polynomial exists.

\vpara
We can verify that $\pi$ is not an algebraic number of degree 3 with coefficients less than 1000:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> findpoly(pi, 3)
>>>
\end{lstlisting}


It is always possible to find an algebraic approximation of a number using one (or several) of the following methods:

\vpara
1. Increasing the permitted degree

2. Allowing larger coefficients

3. Reducing the tolerance

\vpara
One example of each method is shown below:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 15
>>> findpoly(pi, 4)
[95, -545, 863, -183, -298]
>>> findpoly(pi, 3, maxcoeff=10000)
[836, -1734, -2658, -457]
>>> findpoly(pi, 3, tol=1e-7)
[-4, 22, -29, -2]
\end{lstlisting}


It is unknown whether Euler's constant is transcendental (or even irrational). We can use findpoly() to check that if is an algebraic number, its minimal polynomial must have degree at least 7 and a coefficient of magnitude at least 1000000:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 200
>>> findpoly(euler, 6, maxcoeff=10**6, tol=1e-100, maxsteps=1000)
>>>
\end{lstlisting}


Note that the high precision and strict tolerance is necessary for such high-degree runs, since otherwise unwanted low-accuracy approximations will be detected. It may also be necessary to set maxsteps high to prevent a premature exit (before the coefficient bound has been reached). Running with verbose=True to get an idea what is happeningcan be useful.


\newpage
\section{Integer relations (PSLQ)}

\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{pslq? mpNum[]? list of integers to approximate a function.}
	{x? mpNum? A vector of real numbers $x=[x_0,x_1,\ldots,x_n]$}
	{Keywords? String?  tol=None, maxcoeff=1000, full=False, verbose=False}	
\end{mpFunctionsExtract}


Given a vector of real numbers $x=[x_0,x_1,\ldots,x_n]$, pslq(x) uses the PSLQ algorithm to find a list of integers $[c_0,c_1,\ldots,c_n]$ such that 

\begin{equation}
|c_1x_1 + c_2x_2 + \ldots + c_nx_n| < \text{tol}
\end{equation}

and such that $\text{max}|c_k|<\text{maxcoeff}$. If no such vector exists, pslq() returns None. The tolerance defaults to 3/4 of the working precision.



\subsection{Examples}

Find rational approximations for $\pi$:

\lstset{language={Python}}
\begin{lstlisting}
>>> from mpFormulaPy import *
>>> mp.dps = 15; mp.pretty = True
>>> pslq([-1, pi], tol=0.01)
[22, 7]
>>> pslq([-1, pi], tol=0.001)
[355, 113]
>>> mpf(22)/7; mpf(355)/113; +pi
3.14285714285714
3.14159292035398
3.14159265358979
\end{lstlisting}

Pi is not a rational number with denominator less than 1000:

\lstset{language={Python}}
\begin{lstlisting}
>>> pslq([-1, pi])
>>>
\end{lstlisting}


To within the standard precision, it can however be approximated by at least one rational number with denominator less than $10^{12}$:

\lstset{language={Python}}
\begin{lstlisting}
>>> p, q = pslq([-1, pi], maxcoeff=10**12)
>>> print(p); print(q)
238410049439
75888275702
>>> mpf(p)/q
3.14159265358979
\end{lstlisting}


The PSLQ algorithm can be applied to long vectors. For example, we can investigate the rational (in)dependence of integer square roots:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 30
>>> pslq([sqrt(n) for n in range(2, 5+1)])
>>>
>>> pslq([sqrt(n) for n in range(2, 6+1)])
>>>
>>> pslq([sqrt(n) for n in range(2, 8+1)])
[2, 0, 0, 0, 0, 0, -1]
\end{lstlisting}



\subsection{Machin formulas}

A famous formula for is Machin's,

\begin{equation}
\frac{\pi}{4} = 4 \text{ acot }5 - \text{ acot } 239
\end{equation}

There are actually infinitely many formulas of this type. Two others are

\begin{equation}
\frac{\pi}{4} = \text{ acot } 1
\end{equation}
\begin{equation}
\frac{\pi}{4} = 12 \text{ acot } 49 + 32 \text{ acot } 57 + 5 \text{ acot } 239 + 12 \text{ acot } 110443
\end{equation}


We can easily verify the formulas using the PSLQ algorithm:

\lstset{language={Python}}
\begin{lstlisting}
>>> mp.dps = 30
>>> pslq([pi/4, acot(1)])
[1, -1]
>>> pslq([pi/4, acot(5), acot(239)])
[1, -4, 1]
>>> pslq([pi/4, acot(49), acot(57), acot(239), acot(110443)])
[1, -12, -32, 5, -12]
\end{lstlisting}


We could try to generate a custom Machin-like formula by running the PSLQ algorithm with a few inverse cotangent values, for example acot(2), acot(3) ... acot(10). Unfortunately, there is a linear dependence among these values, resulting in only that dependence being detected, with a zero coefficient for $\pi$:

\lstset{language={Python}}
\begin{lstlisting}
>>> pslq([pi] + [acot(n) for n in range(2,11)])
[0, 1, -1, 0, 0, 0, -1, 0, 0, 0]
\end{lstlisting}


We get better luck by removing linearly dependent terms:

\lstset{language={Python}}
\begin{lstlisting}
>>> pslq([pi] + [acot(n) for n in range(2,11) if n not in (3, 5)])
[1, -8, 0, 0, 4, 0, 0, 0]
\end{lstlisting}


In other words, we found the following formula:

\lstset{language={Python}}
\begin{lstlisting}
>>> 8*acot(2) - 4*acot(7)
3.14159265358979323846264338328
>>> +pi
3.14159265358979323846264338328
\end{lstlisting}





